--- a/AddOn/ZAssembly/adler32vec_arm.s	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZAssembly/adler32vec_arm.s	1695412879.000000000
@@ -0,0 +1,403 @@
+#if defined(VEC_OPTIMIZE)
+
+#ifdef __arm__
+
+#include <arm/arch.h>
+#define KERNEL_SUPPORT_NEON 1   // this is for building dylib, defined so it will build with neon for armv7
+#define BASE 65521	    /* largest prime smaller than 65536 */
+#define NMAX 5552 		/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
+
+// Note: buf should have been 16-byte aligned in the caller function,
+
+// uLong adler32_vec(unsigned int adler, unsigned int sum2, const Bytef* buf, int len) {
+//    unsigned n;
+//    while (len >= NMAX) {
+//        len -= NMAX;
+//        n = NMAX / 16;          /* NMAX is divisible by 16 */
+//        do {
+//            DO16(buf);          /* 16 sums unrolled */
+//            buf += 16;
+//        } while (--n);
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    if (len) {                  /* avoid modulos if none remaining */
+//        while (len >= 16) {
+//            len -= 16;
+//            DO16(buf);
+//            buf += 16;
+//        }
+//        while (len--) {
+//            adler += *buf++;
+//            sum2 += adler;
+//        }
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    return adler | (sum2 << 16); 		/* return recombined sums */
+// }
+
+
+/* 
+	DO16 vectorization:
+	given initial unsigned int sum2 and adler, and a new set of 16 input bytes (x[0:15]), it can be shown that
+	sum2  += (16*adler + 16*x[0] + 15*x[1] + ... + 1*x[15]);
+	adler += (x[0] + x[1] + ... + x[15]);
+
+	therefore, this is what can be done to vectorize the above computation
+	1. 16-byte aligned vector load into q2 (x[0:x15])
+	2. sum2 += (adler<<4);
+	3. vmull.u8 (q9,q8),q2,d2 where d2 = (1,1,1,1...,1), (q9,q8) + 16 16-bit elements x[0:15]
+	4. vmull.u8 (q11,q10),q2,q0 where q0 = (1,2,3,4...,16), (q11,q10) + 16 16-bit elements (16:1)*x[0:15]
+	5. parallel add (with once expansion to 32-bit) (q9,q8) and (q11,q10) all the way to accumulate to adler and sum2 
+
+	In this revision, whenever possible, 2 DO16 loops are combined into a DO32 loop.
+	1. 32-byte aligned vector load into q2,q14 (x[0:x31])
+    2. sum2 += (adler<<5);
+    3. vmull.u8 (4 q registers),(q2,q14),d2 where d2 = (1,1,1,1...,1), (4 q registers) : 32 16-bit elements x[0:31]
+	4. vmull.u8 (4 q registers),(q2,q14),(q0,q15) where q0 = (1,...,32), (4 q regs) : 32 16-bit elements (32:1)*x[0:31]
+    5. parallel add (with once expansion to 32-bit) the pair of (4 q regs) all the way to accumulate to adler and sum2 
+
+	This change improves the performance by ~ 0.55 cycle/uncompress byte on ARM Cortex-A8.
+
+*/
+
+/*
+	MOD implementation:
+	adler%BASE = adler - floor(adler*(1/BASE))*BASE; where (1/BASE) = 0x80078071 in Q47
+	1. vmull.u32   q2,(adler,sum2),(1/BASE)		// *(1/BASE) in Q47
+    2. vshr.u64    q2,q2,#47					// floor function
+    3. vpadd.u32   d4,d4,d5						// merge into a double word in d4
+    4. vmls.u32    (adler,sum2),d4,d3[0]        // (adler,sum2) -= floor[(adler,sum2)/BASE]*BASE
+	 
+*/
+
+#if defined _ARM_ARCH_6			// this file would be used only for armv6 or above
+
+
+	.text
+	.align 2
+	.globl _adler32_vec
+_adler32_vec:
+ 
+#if (!KERNEL_SUPPORT_NEON) || (!defined _ARM_ARCH_7)	// for armv6 or armv7 without neon support
+
+
+	#define	adler			r0
+	#define	sum2			r1
+	#define	buf				r2
+	#define	len				r3	
+	#define	one_by_base		r4
+	#define	base			r5
+	#define nmax			r6
+	#define	t				r12
+	#define	vecs			lr
+	#define	x0				r8
+	#define	x1				r10
+	#define	x2				r11
+	#define	x3				r12
+	#define	zero			r9
+
+	// this macro performs adler/sum2 update for 4 input bytes
+
+	.macro DO4
+	add		sum2, adler, lsl #2				// sum2 += 4*adler;
+	ldr		x0,[buf]						// 4 bytes in 1 32-bit word
+	usada8	adler, x0, zero, adler			// adler += sum(x0:x3)
+	ldrb	x0,[buf], #4					// x0
+	ldrb	x2,[buf,#-2]					// x2
+	ldrb	x1,[buf,#-3]					// x1
+	ldrb	x3,[buf,#-1]					// x3
+	add		sum2, x0, lsl #2				// sum2 += 4*x0
+	add		x3, x3, x1, lsl #1				// x3+2*x1
+	add		sum2, x2, lsl #1				// sum2 += 2*x2
+	add		x3, x1							// x3+3*x1
+	add		sum2, x3						// sum2 += x3+3*x1
+	.endm
+
+	// the following macro cascades 4 DO4 into a adler/sum2 update for 16 bytes
+	.macro DO16
+	DO4										// adler/sum2 update for 4 input bytes
+	DO4										// adler/sum2 update for 4 input bytes
+	DO4										// adler/sum2 update for 4 input bytes
+	DO4										// adler/sum2 update for 4 input bytes
+	.endm
+
+	// the following macro performs adler sum2 modulo BASE
+	.macro	modulo_base
+	umull	x0,x1,adler,one_by_base			// adler/BASE in Q47
+	umull	x2,x3,sum2,one_by_base			// sum2/BASE in Q47
+	lsr		x1, #15							// x1 >> 15 = floor(adler/BASE)
+	lsr		x3, #15							// x3 >> 15 = floor(sum2/BASE)
+	mla		adler, x1, base, adler			// adler %= base;
+	mla		sum2, x3, base, sum2			// sum2 %= base;
+	.endm
+
+	adr		t, coeffs	
+	push	{r4-r6, r8-r11, lr}
+	ldmia	t, {one_by_base, base, nmax}	// load up coefficients
+
+	subs        len, nmax                   // pre-subtract len by NMAX
+	eor			zero, zero					// a dummy zero register to use usada8 instruction
+    blt         len_lessthan_NMAX           // if (len < NMAX) skip the while loop     
+
+while_lengenmax_loop:						// do {
+    lsr         vecs, nmax, #4              // vecs = NMAX/16;
+
+len16_loop:									// do {
+
+	DO16
+
+	subs	vecs, #1						// vecs--;
+	bgt			len16_loop					// } while (vec>0);	
+
+	modulo_base								// adler sum2 modulo BASE
+
+	subs		len, nmax					// len -= NMAX
+	bge			while_lengenmax_loop		// } while (len >= NMAX);
+
+len_lessthan_NMAX:
+	adds		len, nmax					// post-subtract len by NMAX
+
+	subs		len, #16					// pre-decrement len by 16
+	blt			len_lessthan_16
+
+len16_loop2:
+
+	DO16
+
+	subs		len, #16
+	bge			len16_loop2
+
+len_lessthan_16:
+	adds		len, #16					// post-increment len by 16
+	beq			len_is_zero
+
+remaining_buf:
+	ldrb		x0, [buf], #1
+	subs		len, #1
+	add			adler, x0
+	add			sum2, adler
+	bgt			remaining_buf
+
+len_is_zero:
+
+	modulo_base 							// adler sum2 modulo BASE
+
+	add		r0, adler, sum2, lsl #16		// to return sum2<<16 | adler 
+
+	pop		{r4-r6, r8-r11, pc}
+
+	.align 2
+coeffs:
+	.long	-2146992015
+	.long	-BASE
+	.long	NMAX
+
+#else	// KERNEL_SUPPORT_NEON
+
+
+#if defined _ARM_ARCH_7 
+
+	#define	adler	r0
+	#define	sum2	r1
+	#define	buf		r2
+	#define	len		r3	
+	#define	nmax	r4
+	#define	nvec	r6				// vecs = NMAX/16
+	#define	n		r5
+	#define	t		r12
+
+	#define	adlersum2		d10
+
+	adr			t, vec_table				// address to vec_table[]
+	stmfd		sp!, {r4, r5, r6, lr}
+	ldr			nmax, [t, #40]				// NMAX
+
+#if	KERNEL
+	vpush	{q12-q15}
+	vpush	{q8-q9}
+	vpush	{q0-q3}
+#endif
+	vpush	{q4-q5}
+
+	vmov		adlersum2, adler, sum2		// pack up adler/sum2 into a double register 
+	cmp			len, nmax					// len vs NMAX
+	vld1.32		{q8-q9},[t,:128]!			// loading up coefficients for adler/sum2 computation
+	vldr		d8,[t]						// for sum2 computation
+	blt			L_len_lessthan_NMAX			// if (len < NMAX) skip the while loop		
+
+	sub			len, len, nmax					// pre-decrement len by NMAX
+
+L_while_len_ge_NMAX_loop: 					// while (len>=NMAX) {
+
+	// DO16
+	vld1.32		{q0},[buf,:128]!			// 16 bytes input
+	vshl.u64	d11, adlersum2, #(4+32)		// adler*16
+	vmull.u8	q12, d0, d18				// 16*x0, 15*x1, ..., 9*x7
+	vmull.u8	q13, d1, d19				// 8*x8, 7*x9, ..., 1*x15
+	vpaddl.u8	q0, q0						// x0+x1, x2+x3, ... x14+x15
+	vadd.i64	adlersum2, adlersum2, d11	// sum2 += adler * 16
+	vadd.i16	q1, q12, q13				// 16*x0+8*x8, ..., 9*x7+1*x15
+	vpaddl.u16	q0, q0						// x0+x1+x2+x3, ..., x12+x13+x14+x15
+	vpaddl.u16	q1, q1						// 16*x0+15*x1+14*x2+13*x3, ... , 4*x12+3*x13+2*x14+x15
+	vpaddl.u32	q0, q0						// x0+...+x7,x8+...+x15
+	vpaddl.u32	q1, q1						// 16*x0+15*x1+...+9*x7, 8*x8+ ... + 1*x15
+	vzip.32		q0, q1						//
+	mov			nvec, #173
+	vadd.i64	d0, d0, d2					//
+
+0:
+
+	vadd.i64	adlersum2, adlersum2, d0
+	vld1.32		{q0-q1},[buf,:128]!			// 32 bytes input
+	vshl.u64	d11, adlersum2, #(5+32)		// adler*32
+	vmull.u8	q12, d0, d16				// 32*x0, 31*x1, ..., 25*x7
+	vmull.u8	q13, d1, d17				// 24*x8, 23*x9, ..., 17*x15
+	vpaddl.u8	q0, q0						// x0+x1, x2+x3, ... x14+x15
+	vmull.u8	q14, d2, d18				// 16*x16, 15*x17, ..., 9*x23
+	vmull.u8	q15, d3, d19				// 8*x24, 7*x25, ..., 1*x31
+	vpaddl.u8	q1, q1						// x16+x17, x18+x19, ... x30+x31
+	vadd.i64	adlersum2, adlersum2, d11	// sum2 += adler * 32
+	vadd.i16    q12, q12, q13
+	vadd.i16    q14, q14, q15
+	vadd.i16    q0, q0, q1
+	vadd.i16    q1, q12, q14
+	subs		nvec, nvec, #1
+	vpaddl.u16	q0, q0						// x0+x1+x2+x3, ..., x12+x13+x14+x15
+	vpaddl.u16	q1, q1						// 16*x0+15*x1+14*x2+13*x3, ... , 4*x12+3*x13+2*x14+x15
+	vpaddl.u32	q0, q0						// x0+...+x7,x8+...+x15
+	vpaddl.u32	q1, q1						// 16*x0+15*x1+...+9*x7, 8*x8+ ... + 1*x15
+	vzip.32		q0, q1						//
+	vadd.i64	d0, d0, d2					//
+	bgt			0b
+	vadd.i64	adlersum2, adlersum2, d0
+
+	// mod(alder,BASE); mod(sum2,BASE);
+	vmull.u32	q2,adlersum2,d8[1]			// alder/BASE, sum2/BASE in Q47
+	subs		len, len, nmax					// len -= NMAX;
+	vshr.u64	q2,q2,#47					// take the integer part
+	vpadd.u32	d4,d4,d5					// merge into a double word in d4
+	vmls.u32	adlersum2,d4,d8[0]			// (adler,sum2) -= floor[(adler,sum2)/BASE]*BASE
+
+	bge			L_while_len_ge_NMAX_loop		// repeat while len >= NMAX
+
+	adds		len, len, nmax					// post-increment len by NMAX
+
+L_len_lessthan_NMAX:
+
+	subs		len, len, #32					// pre-decrement len by 32
+	blt			L_len_lessthan_32				// if len < 32, branch to len16_loop 
+
+L_len32_loop:
+	vld1.32		{q0-q1},[buf,:128]!			// 32 bytes input
+	vshl.u64	d11, adlersum2, #(5+32)		// adler*32
+	vmull.u8	q12, d0, d16				// 32*x0, 31*x1, ..., 25*x7
+	vmull.u8	q13, d1, d17				// 24*x8, 23*x9, ..., 17*x15
+	vpaddl.u8	q0, q0						// x0+x1, x2+x3, ... x14+x15
+	vmull.u8	q14, d2, d18				// 16*x16, 15*x17, ..., 9*x23
+	vmull.u8	q15, d3, d19				// 8*x24, 7*x25, ..., 1*x31
+	vpaddl.u8	q1, q1						// x16+x17, x18+x19, ... x30+x31
+	vadd.i64	adlersum2, adlersum2, d11	// sum2 += adler * 32
+	vadd.i16    q12, q12, q13
+	vadd.i16    q14, q14, q15
+	vadd.i16    q0, q0, q1
+	vadd.i16    q1, q12, q14
+	subs		len, len, #32					// len -= 32; 
+	vpaddl.u16	q0, q0						// x0+x1+x2+x3, ..., x12+x13+x14+x15
+	vpaddl.u16	q1, q1						// 16*x0+15*x1+14*x2+13*x3, ... , 4*x12+3*x13+2*x14+x15
+	vpaddl.u32	q0, q0						// x0+...+x7,x8+...+x15
+	vpaddl.u32	q1, q1						// 16*x0+15*x1+...+9*x7, 8*x8+ ... + 1*x15
+	vzip.32		q0, q1						//
+	vadd.i64	d0, d0, d2					//
+	vadd.i64	adlersum2, adlersum2, d0
+	bge			L_len32_loop
+
+L_len_lessthan_32:
+
+	tst         len, #16
+	beq			1f
+	vld1.32		{q0},[buf,:128]!			// 16 bytes input
+	vshl.u64	d11, adlersum2, #(4+32)		// adler*16
+	vmull.u8	q12, d0, d18				// 16*x0, 15*x1, ..., 9*x7
+	vmull.u8	q13, d1, d19				// 8*x8, 7*x9, ..., 1*x15
+	vpaddl.u8	q0, q0						// x0+x1, x2+x3, ... x14+x15
+	vadd.i64	adlersum2, adlersum2, d11	// sum2 += adler * 16
+	vadd.i16	q1, q12, q13				// 16*x0+8*x8, ..., 9*x7+1*x15
+	vpaddl.u16	q0, q0						// x0+x1+x2+x3, ..., x12+x13+x14+x15
+	vpaddl.u16	q1, q1						// 16*x0+15*x1+14*x2+13*x3, ... , 4*x12+3*x13+2*x14+x15
+	vpaddl.u32	q0, q0						// x0+...+x7,x8+...+x15
+	vpaddl.u32	q1, q1						// 16*x0+15*x1+...+9*x7, 8*x8+ ... + 1*x15
+	vzip.32		q0, q1						//
+	vadd.i64	d0, d0, d2					//
+	vadd.i64	adlersum2, adlersum2, d0
+1:
+	ands		len, len, #15				// post-increment len by 16
+	beq			L_len_is_zero_internal		// if len==0, branch to len_is_zero_internal
+
+	// restore adler/sum2 into general registers for remaining (<16) bytes
+
+	vmov		adler, sum2, adlersum2
+L_remaining_len_loop:
+	ldrb		t, [buf], #1				// *buf++;
+	subs		len, #1						// len--;
+	add			adler,t						// adler += *buf
+	add			sum2,adler					// sum2 += adler
+	bgt			L_remaining_len_loop			// break if len<=0
+
+	vmov		adlersum2, adler, sum2		// move to double register for modulo operation
+
+L_len_is_zero_internal:
+
+	// mod(alder,BASE); mod(sum2,BASE);
+
+	vmull.u32	q2,adlersum2,d8[1]			// alder/BASE, sum2/BASE in Q47
+	vshr.u64	q2,q2,#47					// take the integer part
+	vpadd.u32	d4,d4,d5					// merge into a double word in d4
+	vmls.u32	adlersum2,d4,d8[0]			// (adler,sum2) -= floor[(adler,sum2)/BASE]*BASE
+
+	vmov        adler, sum2, adlersum2		// restore adler/sum2 from (s12=sum2, s13=adler)
+	add			r0, adler, sum2, lsl #16	// to return adler | (sum2 << 16);
+
+	vpop		{q4-q5}
+#if	KERNEL
+	vpop		{q0-q1}
+	vpop		{q2-q3}
+	vpop		{q8-q9}
+	vpop		{q12-q13}
+	vpop		{q14-q15}
+#endif
+
+	ldmfd       sp!, {r4, r5, r6, pc}			// restore registers and return 
+
+
+	// constants to be loaded into q registers
+	.align	4		// 16 byte aligned
+
+vec_table:
+
+	// coefficients for computing sum2
+	.long	0x1d1e1f20		// s0
+	.long	0x191a1b1c		// s1
+	.long	0x15161718		// s2
+	.long	0x11121314		// s3
+	.long	0x0d0e0f10		// s0
+	.long	0x090a0b0c		// s1
+	.long	0x05060708		// s2
+	.long	0x01020304		// s3
+
+	.long	BASE			// s6 : BASE 
+	.long	0x80078071		// s7 : 1/BASE in Q47
+
+NMAX_loc:
+	.long	NMAX			// NMAX
+	
+#endif		// _ARM_ARCH_7
+
+#endif		//  (!KERNEL_SUPPORT_NEON) || (!defined _ARM_ARCH_7)
+
+#endif		// _ARM_ARCH_6
+
+#endif		// __arm__
+
+#endif		// VEC_OPTIMIZE
--- a/AddOn/ZAssembly/adler32vec_arm64.s	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZAssembly/adler32vec_arm64.s	1695412879.000000000
@@ -0,0 +1,304 @@
+#if defined(VEC_OPTIMIZE) && defined(__arm64__)
+
+#define BASE 65521	    /* largest prime smaller than 65536 */
+#define NMAX 5552 		/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
+
+// Note: buf should have been 16-byte aligned in the caller function,
+
+// uLong adler32_vec(unsigned int adler, unsigned int sum2, const Bytef* buf, int len) {
+//    unsigned n;
+//    while (len >= NMAX) {
+//        len -= NMAX;
+//        n = NMAX / 16;          /* NMAX is divisible by 16 */
+//        do {
+//            DO16(buf);          /* 16 sums unrolled */
+//            buf += 16;
+//        } while (--n);
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    if (len) {                  /* avoid modulos if none remaining */
+//        while (len >= 16) {
+//            len -= 16;
+//            DO16(buf);
+//            buf += 16;
+//        }
+//        while (len--) {
+//            adler += *buf++;
+//            sum2 += adler;
+//        }
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    return adler | (sum2 << 16); 		/* return recombined sums */
+// }
+
+
+/* 
+	DO16 vectorization:
+	given initial unsigned int sum2 and adler, and a new set of 16 input bytes (x[0:15]), it can be shown that
+	sum2  += (16*adler + 16*x[0] + 15*x[1] + ... + 1*x[15]);
+	adler += (x[0] + x[1] + ... + x[15]);
+
+	similarly, 2 DO16 = DO32 vectorization:
+	given initial unsigned int sum2 and adler, and a new set of 32 input bytes (x[0:31]), it can be shown that
+	sum2  += (32*adler + 32*x[0] + 31*x[1] + ... + 1*x[31]);
+	adler += (x[0] + x[1] + ... + x[31]);
+
+	NMAX = 5552 
+	NMAX/16 = 347 = 173*2 + 1
+
+	therefore, for a block of 5552 bytes
+
+		n = 173;
+		do {
+			DO32(buf); buf+=32;
+		} while (n--);
+
+		DO16(buf); buf+=16;
+		MOD(adler);
+		MOD(sum2);
+
+	for a residual remaining len bytes,
+		while (len >= 32) {
+			DO32(buf); buf += 32; len -= 32;
+        }
+		if (len>=16) {
+			DO16(buf); buf += 16; len -= 16;
+		}
+        while (len--) {
+            adler += *buf++;
+            sum2 += adler;
+        }
+        MOD(adler);
+        MOD(sum2);
+
+		
+
+
+	DO32:
+	pack sum2:adler in a 64-bit register
+
+	0. sum2 += adler*32;	
+
+		sum2:adler += (sum2:adler) << (32+5);
+
+	1. (32, 31, ..., 1) * ( x0, x1, ..., x31)	(v2, v3) * (v0, v1)
+		umull.8h	v4, v2, v0
+		umull2.8h	v4, v2, v0
+		umull.8h	v4, v3, v1
+		umull2.8h	v4, v3, v1
+		uaddlv		s4, v4.8h
+
+		add		sum2, sum2, s4
+
+	2.	adler += (x0 + x1 + x2 + ... + x31)
+
+		uaddlp		v0.8h, v0.16b 
+		uaddlp		v1.8h, v1.16b 
+		uaddlv		s0, v0.8h
+		uaddlv		s1, v1.8h
+		add			s0, s0, s1		
+
+		add		adler, adler, s0
+	
+
+	therefore, this is what can be done to vectorize the above computation
+	1. 16-byte aligned vector load into q2 (x[0:x15])
+	2. sum2 += (adler<<32);				// pack adler/sum2 into a 64-bit word
+	3. vmull.u8 (q9,q8),q2,d2 where d2 = (1,1,1,1...,1), (q9,q8) + 16 16-bit elements x[0:15]
+	4. vmull.u8 (q11,q10),q2,q0 where q0 = (1,2,3,4...,16), (q11,q10) + 16 16-bit elements (16:1)*x[0:15]
+	5. parallel add (with once expansion to 32-bit) (q9,q8) and (q11,q10) all the way to accumulate to adler and sum2 
+
+	In this revision, whenever possible, 2 DO16 loops are combined into a DO32 loop.
+	1. 32-byte aligned vector load into q2,q14 (x[0:x31])
+    2. sum2 += (adler<<32);
+    3. vmull.u8 (4 q registers),(q2,q14),d2 where d2 = (1,1,1,1...,1), (4 q registers) : 32 16-bit elements x[0:31]
+	4. vmull.u8 (4 q registers),(q2,q14),(q0,q15) where q0 = (1,...,32), (4 q regs) : 32 16-bit elements (32:1)*x[0:31]
+    5. parallel add (with once expansion to 32-bit) the pair of (4 q regs) all the way to accumulate to adler and sum2 
+
+	This change improves the performance by ~ 0.55 cycle/uncompress byte on ARM Cortex-A8.
+
+*/
+
+/*
+	MOD implementation:
+	adler%BASE = adler - floor(adler*(1/BASE))*BASE; where (1/BASE) = 0x80078071 in Q47
+	1. vmull.u32   q2,(adler,sum2),(1/BASE)		// *(1/BASE) in Q47
+    2. vshr.u64    q2,q2,#47					// floor function
+    3. vpadd.u32   d4,d4,d5						// merge into a double word in d4
+    4. vmls.u32    (adler,sum2),d4,d3[0]        // (adler,sum2) -= floor[(adler,sum2)/BASE]*BASE
+	 
+*/
+
+
+	.text
+	.align 2
+	.globl _adler32_vec
+_adler32_vec:
+ 
+
+
+	#define	adler	w0
+	#define	sum2	w1
+	#define	buf		x2
+	#define	len		w3	
+	#define	nmax	w4
+	#define	nvec	w6				// vecs = NMAX/16
+
+	#define	t		x7
+
+	#define	vadlersum2		v5			// only lower part 
+	#define	adlersum2		d5			// only lower part 
+
+	add		x0, x0, x1, lsl #32
+	adrp    t, vec_table@PAGE
+	mov			nmax, #NMAX					// NMAX
+    add     t, t, vec_table@PAGEOFF
+	ins		vadlersum2.d[0], x0
+
+
+#if	KERNEL
+	sub		x6, sp, #8*16	
+	sub		sp, sp, #8*16	
+	st1.4s	{v0,v1,v2,v3},[x6],#4*16
+	st1.4s	{v4,v5,v6,v7},[x6],#4*16
+#endif
+
+	cmp			len, nmax					// len vs NMAX
+	ld1.4s		{v0, v1}, [t], #2*16 		// loading up coefficients for adler/sum2 computation
+	ldr			d7, [t]						// for MOD operation
+	b.lt		L_len_lessthan_NMAX			// if (len < NMAX) skip the while loop		
+
+	sub			len, len, nmax				// pre-decrement len by NMAX
+
+L_while_len_ge_NMAX_loop: 					// while (len>=NMAX) {
+
+
+	// 5552/16 = 173*2 + 1, need 1 DO16 + 173 DO32
+
+	.macro	DO16
+	ld1.4s	{v2},[buf]	 					// 16 bytes input
+	shl		d6, adlersum2, #(4+32)			// adler * 16 
+	umull.8h    v4, v2, v1					// 16*x0, 15*x1, ..., 9*x7 
+	add		adlersum2, adlersum2, d6		// sum2 += adler * 16
+    umlal2.8h   v4, v2, v1					// 8*x8, 7*x9, ..., 1*x15 
+    uaddlv      h2, v2.16b					// x0+x1+x2+...+x15
+    uaddlv      s4, v4.8h					// 16*x0 + 15*x1 + ... + 1*x15
+	zip1.2s		v4, v2, v4
+	add			buf, buf, #16
+	add.2s		vadlersum2, vadlersum2, v4
+	.endm
+
+	.macro	DO32
+	ld1.4s		{v2,v3},[buf]					// 32 bytes input
+	shl			d6, adlersum2, #(5+32)			// adler * 32 
+	umull.8h    v4, v2, v0						// 32*x0, 31*x1, ..., 25*x7 
+	add			adlersum2, adlersum2, d6		// sum2 += adler * 32 
+    umlal2.8h   v4, v2, v0						// accumulate 24*x8, 23*x9, ..., 17*x15 
+    uaddlv      h2, v2.16b						// x0+x1+x2+...+x15 and extend from byte to short
+	umlal.8h    v4, v3, v1						// accumulate 16*x16, 15*x17, ..., 9*x23 
+    uaddlv      h6, v3.16b						// x16+x17+x18+...+x31 and extend from byte to short
+    umlal2.8h   v4, v3, v1						// accumulate 8*x24, 7*x25, ..., 1*x31 
+	uaddl.4s	v2, v2, v6						// x0+x1+...+x31 and extend from short to int
+	uaddlv		s4, v4.8h						// 32*x0 + 31*x1 + ... + 1*x31
+	zip1.2s		v4, v2, v4
+	add			buf, buf, #32
+	add.2s		vadlersum2, vadlersum2, v4
+	.endm
+
+	.macro	MOD_BASE
+	umull.2d	v2,vadlersum2,v7[1]
+	ushr.2d		v2, v2, #47
+	ins			v2.s[1], v2.s[2]
+	mls.2s		vadlersum2,v2,v7[0]
+	.endm
+
+	DO16
+	mov			nvec, #173						// number of DO32 loops
+
+0:
+	DO32
+
+	subs		nvec, nvec, #1	
+	b.gt		0b
+
+	MOD_BASE	 // MOD(adler,sum2);
+	
+	subs		len, len, nmax				// pre-decrement len by NMAX
+	b.ge		L_while_len_ge_NMAX_loop	// repeat while len >= NMAX
+
+	adds		len, len, nmax				// post-increment len by NMAX
+
+L_len_lessthan_NMAX:
+
+	subs		len, len, #32					// pre-decrement len by 32
+	b.lt		L_len_lessthan_32				// if len < 32, branch to len16_loop 
+
+L_len32_loop:
+
+	DO32
+
+	subs		len, len, #32					// len -= 32; 
+	b.ge		L_len32_loop
+
+L_len_lessthan_32:
+
+	tst			len, #16
+	b.eq		1f
+
+	DO16
+
+1:
+	ands		len, len, #15
+	b.eq		L_len_is_zero
+
+
+L_remaining_len_loop:
+	ldrb		w0, [buf], #1				// *buf++;
+	sub			len, len, #1
+	ins			v2.d[0], x0
+	
+	add			adlersum2, adlersum2, d2	// adler += *buf
+	shl			d6, adlersum2, #32			// shift adler up to sum2 position 
+	add			adlersum2, adlersum2, d6	// sum2 += adler
+	cbnz		len, L_remaining_len_loop	// break if len<=0
+
+
+L_len_is_zero:
+
+
+	MOD_BASE 	// mod(alder,BASE); mod(sum2,BASE);
+
+	umov		w0, vadlersum2.s[0]			// adler
+	umov		w1, vadlersum2.s[1]			// sum2
+	add			w0, w0, w1, lsl #16	// to return adler | (sum2 << 16);
+
+#if	KERNEL
+	ld1.4s	{v0,v1,v2,v3},[sp],#4*16
+	ld1.4s	{v4,v5,v6,v7},[sp],#4*16
+#endif
+
+	ret			lr
+
+	// constants to be loaded into q registers
+	.align	4		// 16 byte aligned
+
+vec_table:
+
+	// coefficients for computing sum2
+	.long	0x1d1e1f20		// s0
+	.long	0x191a1b1c		// s1
+	.long	0x15161718		// s2
+	.long	0x11121314		// s3
+	.long	0x0d0e0f10		// s0
+	.long	0x090a0b0c		// s1
+	.long	0x05060708		// s2
+	.long	0x01020304		// s3
+
+	.long	BASE			// s6 : BASE 
+	.long	0x80078071		// s7 : 1/BASE in Q47
+
+	
+
+#endif		// VEC_OPTIMIZE && __arm64__
--- a/AddOn/ZAssembly/adler32vec_intel.s	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZAssembly/adler32vec_intel.s	1695412879.000000000
@@ -0,0 +1,1066 @@
+#if defined(VEC_OPTIMIZE)
+
+/* Apple Copyright 2009
+   CoreOS - vector & Numerics, cclee 10-22-09
+
+	This following source code implements a vectorized version of adler32 computation that is defined in zlib.
+	The target architectures are x86_64 and i386.
+
+	Given 2 unsigned 32-bit alder and sum2 (both pre-modulo by BASE=65521) and a sequence of input bytes x[0],...x[N-1].
+	The adler-sum2 pair is updated according to
+
+		for (i=0;i<N;i++) {
+			adler = (adler+x[i])%BASE;
+			sum2 = (sum2+adler)%BASE;
+		}
+
+	To reduce/save the modulo operations, it can be shown that, if initial alder and sum2 are less than BASE(=65521),
+	adler and sum2 (in 32-bit representation), will never overflow for the next NMAX=5552 bytes. This simplifies the
+	algorithm to 
+
+		for (i=0;i<N;i+=NMAX) {
+			for (k=0;k<NMAX;k++) {
+				adler+=x[i+k];
+				sum2+=adler;
+			}
+			adler%=BASE;
+			sum2%=BASE;
+		}
+
+	The hand optimization of this function is now reduced to 
+
+			for (k=0;k<NMAX;k++) {
+                adler+=x[k];
+                sum2+=adler;
+            }
+
+	This subtask turns out to be very vecterizable. Suppose we perform the adler/sum2 update once per K bytes,
+
+			for (k=0;k<K;k++) {
+                adler+=x[k];
+                sum2+=adler;
+            }
+
+	It can be shown that the sum2-adler pair can be updated according to
+
+		sum2 += adler*K;
+		adler += (x[0] + x[1] + ... + x[K-1]); 
+		sum2 += (x[0]*K + x[1]*(K-1) + ... + x[K-1]*1);
+
+	The last 2 equations obviously show that the adler-sum2 pair update can be speeded up using vector processor.
+	The input vector [ x[0] x[1] ... x[K-1] ]. And we need two coefficient vectors
+		[ 1 1 1 ... 1 ] for adler update.
+		[ K K-1 ... 1 ] for sum2 update.
+
+	The implementation below reads vector (K=16,32,48,64) into xmm registers, and sets up coefficient vectors in xmm
+	registers. It then uses SSE instructions to perform the aforementioned vector computation.
+
+	For i386, NMAX/16 = 347, whenever possible (NMAX-bytes block), it calls 173 times of macro code DO32 (K=32),
+	followed by a single DO16 (K=16), before calling a modulo operation for adler and sum2.
+
+	For x86_64 (where more xmm registers are available), NMAX/64 = 86, whenever possible (NMAX-bytes block), 
+	it calls 86 times of macro code DO64 (K=64), followed by a single DO48 (K=48), 
+	before calling a modulo operation for adler and sum2.
+
+*/
+
+/* added cpu_capability to detect kHasSupplementalSSE3 to branch into code w or wo SupplementalSSE3
+
+	Previously, ssse3 code was intentionally turned off, because Yonah does not support ssse3
+	add code here to probe cpu_capabilities for ssse3 support
+		if ssse3 is supported, branch to ssse3-based code, otherwise use the original code
+
+	cclee 5-3-10
+*/
+
+#define BASE 65521  /* largest prime smaller than 65536 */
+#define NMAX 5552 	/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
+
+// uLong	adler32_vec(unsigned int adler, unsigned int sum2, const Bytef *buf, int len) {
+//    unsigned n;
+//    while (len >= NMAX) {
+//        len -= NMAX;
+//        n = NMAX / 16;          /* NMAX is divisible by 16 */
+//        do {
+//            DO16(buf);          /* 16 sums unrolled */
+//            buf += 16;
+//        } while (--n);
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    if (len) {                  /* avoid modulos if none remaining */
+//        while (len >= 16) {
+//            len -= 16;
+//            DO16(buf);
+//            buf += 16;
+//        }
+//        while (len--) {
+//            adler += *buf++;
+//            sum2 += adler;
+//        }
+//        MOD(adler);
+//        MOD(sum2);
+//    }
+//    return adler | (sum2 << 16);
+// }
+
+#if (defined __i386__ || defined __x86_64__)
+
+#include <machine/cpu_capabilities.h>
+
+	.text
+	.align 4,0x90
+.globl _adler32_vec
+_adler32_vec:
+
+#if (defined __i386__)
+
+	pushl	%ebp
+	movl	%esp, %ebp
+
+	pushl	%ebx
+	pushl	%edi
+	pushl	%esi
+
+#ifdef	KERNEL 						// if this is for kernel, need to save xmm registers
+	subl	$140, %esp				// to save %xmm0-%xmm7 into stack, extra 12 to align %esp to 16-byte boundary
+	movaps	%xmm0, 0(%esp)		// save xmm0, offset -12 for ebx/edi/esi
+	movaps	%xmm1, 16(%esp)		// save xmm1
+	movaps	%xmm2, 32(%esp)		// save xmm2
+	movaps	%xmm3, 48(%esp)		// save xmm3
+	movaps	%xmm4, 64(%esp)		// save xmm4
+	movaps	%xmm5, 80(%esp)		// save xmm5
+	movaps	%xmm6, 96(%esp)		// save xmm6
+	movaps	%xmm7, 112(%esp)		// save xmm7, if this is for SSSE3 or above
+#endif
+
+	#define	adler	%edi				// 8(%ebp)
+	#define	sum2	%esi				// 12(%ebp)
+	#define	buf		%ecx				// 16(%ebp)
+	#define	len		%ebx				// 20(%ebp)
+	#define	zero	%xmm0
+	#define ones	%xmm5
+
+	movl	8(%ebp), adler
+	movl	12(%ebp), sum2
+	movl	16(%ebp), buf			// use ecx as buf pointer
+	movl	20(%ebp), len
+
+	.macro		modulo_BASE
+	movl		$$-2146992015, %eax		// 1/BASE in Q47
+	mull		adler					// edx:eax = adler divided by BASE in Q47
+	shrl		$$15, %edx				// edx is now the floor integer of adler and BASE
+	imull		$$BASE, %edx, %edx		// edx * BASE
+	subl		%edx, adler				// adler -= edx*BASE
+	movl		$$-2146992015, %eax		// 1/BASE in Q47
+	mull		sum2					// edx:eax = sum2 divided by BASE in Q47
+	shrl		$$15, %edx				// edx is now the floor integer of sum2 and BASE
+	imull		$$BASE, %edx, %eax		// eax = edx * BASE
+	subl		%eax, sum2				// sum2 -= sdx*BASE
+	.endmacro
+
+	// update adler/sum2 according to a new 16-byte vector
+	.macro		DO16
+	movaps		(buf), %xmm1			// 16 bytes vector, in xmm1
+	movaps		%xmm1, %xmm3			// a copy of the vector, used for unsigned byte in the destination of pmaddubsw
+	addl		$$16, buf				// buf -> next vector
+	psadbw		zero, %xmm1				// 2 16-bit words to be added for adler in xmm1
+	pmaddubsw	%xmm4, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	imull		$$16, adler, %edx		// edx = 16*adler;
+	movhlps		%xmm1, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+	paddq		%xmm2, %xmm1			// xmm1 lower 32-bit to be added to adler
+	addl		%edx, sum2				// sum2 += adler*16;
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm1, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addl		%edx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addl		%edx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addl		%edx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// update adler/sum2 according to a new 32-byte vector
+	.macro		DO32
+	imull		$$32, adler, %edx		// edx = 32*adler
+	movaps		(buf), %xmm1			// 1st 16 bytes vector
+	movaps		16(buf), %xmm7			// 2nd 16 bytes vector
+	movaps		%xmm1, %xmm3			// a copy of 1st vector, used for unsigned byte in the destination of pmaddubsw
+	movaps		%xmm7, %xmm2			// a copy of 2nd vector, used for unsigned byte in the destination of pmaddubsw
+	psadbw		zero, %xmm1				// 2 16-bit words to be added for adler in xmm1
+	psadbw		zero, %xmm7				// 2 16-bit words to be added for adler in xmm7
+	addl		%edx, sum2				// sum2 += adler*32;
+	pmaddubsw	%xmm6, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	pmaddubsw	%xmm4, %xmm2			// 8 16-bit words to be added for sum2 in xmm2
+	paddd		%xmm7, %xmm1			// 2 16-bit words to be added for adler in xmm1
+	paddd		%xmm2, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	addl		$$32, buf				// buf -> vector for next iteration
+	movhlps		%xmm1, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+	paddq		%xmm2, %xmm1			// xmm1 lower 32-bit to be added to adler
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm1, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addl		%edx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addl		%edx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addl		%edx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// this defines the macro DO16 for SSSE3 not supported
+    .macro      DO16_nossse3
+    movaps      (buf), %xmm1            // 16 bytes vector
+    movaps      %xmm1, %xmm3            // a copy of the vector, the lower 8 bytes to be shuffled into 8 words
+    movaps      %xmm1, %xmm2            // a copy of the vector, the higher 8 bytes to be shuffled into 8 words
+    psrldq      $$8, %xmm2              // shift down 8 bytes, to reuse the shuffle vector
+    punpcklbw   zero, %xmm3             // convert lower 8 bytes into 8 words
+    punpcklbw   zero, %xmm2             // convert higher 8 bytes into 8 words
+    pmullw      %xmm6, %xmm3            // lower 8 words * 16:9
+    pmullw      %xmm4, %xmm2            // higher 8 words * 8:1
+    addl        $$16, buf               // buf -> next vector
+    psadbw      zero, %xmm1             // 2 16-bit words to be added for adler in xmm1
+    paddw       %xmm2, %xmm3            // 8 16-bit words to be added for sum2 in xmm3
+    imull       $$16, adler, %edx       // edx = 16*adler;
+    movhlps     %xmm1, %xmm2            // higher 16-bit word (for adler) in xmm2   
+    pmaddwd     ones, %xmm3             // 4 32-bit elements to be added for sum2 in xmm3
+    paddq       %xmm2, %xmm1            // xmm1 lower 32-bit to be added to adler
+    addl        %edx, sum2              // sum2 += adler*16;
+    movhlps     %xmm3, %xmm2            // 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+    movd        %xmm1, %edx             // to be added to adler
+    paddd       %xmm2, %xmm3            // 2 32-bits elements in xmm3 to be added to sum2
+    addl        %edx, adler             // update adler
+    movd        %xmm3, %edx             // to be added to sum2
+    psrlq       $$32, %xmm3             // another 32-bit to be added to sum2
+    addl        %edx, sum2              // sum2 += 1st half of update
+    movd        %xmm3, %edx             // to be added to sum2
+    addl        %edx, sum2              // sum2 += 2nd half of update
+    .endm
+
+#ifdef  KERNEL
+    leal    __cpu_capabilities, %eax                        // %eax -> __cpu_capabilities
+    mov     (%eax), %eax                                    // %eax = __cpu_capabilities
+#else
+    mov    _COMM_PAGE_CPU_CAPABILITIES, %eax
+#endif
+    test    $(kHasSupplementalSSE3), %eax 					// __cpu_capabilities & kHasAES
+	je		L_no_ssse3
+
+	// i386 adler32 with ssse3
+
+	// need to fill up xmm4/xmm5/xmm6 only if len>=16
+	cmpl	$16, len
+	jl		L_skip_loading_tables
+
+	// set up table starting address to %eax
+#ifdef	KERNEL
+	leal	sum2_coefficients, %eax
+#else
+	call	0f
+0:	pop	%eax
+	lea	sum2_coefficients-0b(%eax), %eax
+#endif
+
+	// reading coefficients
+	pxor	zero, zero
+	movaps	(%eax), %xmm6			// coefficients for computing sum2 : pmaddubsw 32:17
+	movaps	16(%eax), %xmm4			// coefficients for computing sum2 : pmaddubsw 16:1
+	movaps	32(%eax), ones			// coefficients for computing sum2 : pmaddwd 1,1,...,1
+
+L_skip_loading_tables:
+
+	cmpl	$NMAX, len				// len vs NMAX
+	jl		len_lessthan_NMAX		// if (len < NMAX), skip the following NMAX batches processing
+
+len_ge_NMAX_loop:					// while (len>=NMAX) {
+
+	subl	$NMAX, len				// 		len -= NMAX
+	movl	$(NMAX/32), %eax		// 		n = NMAX/32
+
+n_loop:								// 		do {
+	DO32							// 			update adler/sum2 for a 32-byte input
+	decl 	%eax					// 			n--;
+	jg		n_loop					//  	} while (n);
+	DO16							//  	update adler/sum2 for a 16-byte input
+	modulo_BASE						// 		(adler/sum2) modulo BASE;
+	cmpl	$NMAX, len				//  
+	jge		len_ge_NMAX_loop		// }	/* len>=NMAX */
+
+len_lessthan_NMAX:
+
+	subl	$32, len				// pre-decrement len by 32
+	jl		len_lessthan_32			// if len < 32, skip the 32-vector code
+len32_loop:							// while (len>=32) {
+	DO32							//   update adler/sum2 for a 32-byte input
+	subl	$32, len				//   len -= 32;
+	jge		len32_loop				// } 
+
+len_lessthan_32:
+
+	addl	$(32-16), len			// post-increment by 32 + pre-decrement by 16 on len
+	jl		L_len_lessthan_16			// if len < 16, skip the 16-vector code
+	DO16							// update adler/sum2 for a 16-byte input
+	subl	$16, len				// len -= 16;
+
+L_len_lessthan_16:
+	addl	$16, len				// post-increment len by 16
+	jz		len_is_zero				// if len==0, branch over scalar processing
+
+0:									// while (len) {
+	movzbl	(buf), %edx				// 	new input byte
+	incl	buf						// 	buf++
+	addl	%edx, adler				// 	adler += *buf
+	addl	adler, sum2				// 	sum2 += adler
+	subl	$1, len					// 	len--
+	jg		0b						// }
+
+len_is_zero:
+
+	modulo_BASE						// (adler/sum2) modulo BASE;
+
+	// construct 32-bit (sum2<<16 | adler) to be returned
+
+	sall	$16, sum2				// sum2 <<16
+	movl	adler, %eax				// adler		
+	orl		sum2, %eax				// sum2<<16 | adler
+
+
+#ifdef	KERNEL 					// if this is for kernel code, need to restore xmm registers
+	movaps	(%esp), %xmm0		// restore xmm0, offset -12 for ebx/edi/esi
+	movaps	16(%esp), %xmm1		// restore xmm1
+	movaps	32(%esp), %xmm2		// restore xmm2
+	movaps	48(%esp), %xmm3		// restore xmm3
+	movaps	64(%esp), %xmm4		// restore xmm4
+	movaps	80(%esp), %xmm5		// restore xmm5
+	movaps	96(%esp), %xmm6		// restore xmm6
+	movaps	112(%esp), %xmm7	// restore xmm7, if this is for SSSE3 or above
+	addl	$140, %esp			// we've already restored %xmm0-%xmm7 from stack
+#endif
+
+    popl   %esi
+    popl   %edi
+	popl   %ebx
+	leave						// pop ebp out from stack
+	ret
+
+
+L_no_ssse3:
+
+	// i386 adler32 without ssse3
+
+	// need to fill up xmm4/xmm5/xmm6 only if len>=16
+	cmpl	$16, len
+	jl		2f
+
+	// set up table starting address to %eax
+#ifdef	KERNEL
+	leal	sum2_coefficients, %eax
+#else
+	call	0f
+0:	pop		%eax
+	lea		sum2_coefficients-0b(%eax), %eax
+#endif
+
+	// reading coefficients
+	pxor	zero, zero
+	movaps  48(%eax), %xmm6         // coefficients for computing sum2 : pmaddubsw 16:9
+    movaps  64(%eax), %xmm4         // coefficients for computing sum2 : pmaddubsw 8:1
+    movaps  80(%eax), ones          // coefficients for computing sum2 : pmaddwd 1,1,...,1
+
+2:
+
+	cmpl	$NMAX, len				// len vs NMAX
+	jl		3f						// if (len < NMAX), skip the following NMAX batches processing
+
+0:									// while (len>=NMAX) {
+
+	subl	$NMAX, len				// 		len -= NMAX
+	movl	$(NMAX/16), %eax		// 		n = NMAX/16
+
+1:									// 		do {
+	DO16_nossse3					//			update adler/sum2 for a 16-byte input
+	decl 	%eax					// 			n--;
+	jg		1b						//  	} while (n);
+
+	modulo_BASE						// 		(adler/sum2) modulo BASE;
+
+	cmpl	$NMAX, len				//  
+	jge		0b						// }	/* len>=NMAX */
+
+3:
+
+	subl	$16, len				// pre-decrement len by 16
+	jl		L_len_lessthan_16		// if len < 16, skip the 16-vector code
+	DO16_nossse3					// update adler/sum2 for a 16-byte input
+	subl	$16, len				// len -= 16;
+	jmp		L_len_lessthan_16
+
+
+	.const
+	.align	4
+sum2_coefficients:	// used for vectorizing adler32 computation
+
+	.byte	32
+	.byte	31
+	.byte	30
+	.byte	29
+	.byte	28
+	.byte	27
+	.byte	26
+	.byte	25
+	.byte	24
+	.byte	23
+	.byte	22
+	.byte	21
+	.byte	20
+	.byte	19
+	.byte	18
+	.byte	17
+	.byte	16
+	.byte	15
+	.byte	14
+	.byte	13
+	.byte	12
+	.byte	11
+	.byte	10
+	.byte	9
+	.byte	8
+	.byte	7
+	.byte	6
+	.byte	5
+	.byte	4
+	.byte	3
+	.byte	2
+	.byte	1
+
+	// coefficients for pmaddwd, to combine into 4 32-bit elements for sum2
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+
+
+	// data for without ssse3
+
+	.word   16
+    .word   15
+    .word   14
+    .word   13
+    .word   12
+    .word   11
+    .word   10
+    .word   9
+    .word   8
+    .word   7
+    .word   6
+    .word   5
+    .word   4
+    .word   3
+    .word   2
+    .word   1
+
+	// coefficients for pmaddwd, to combine into 4 32-bit elements for sum2
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+
+#else	// (defined __x86_64__)
+
+	movq    __cpu_capabilities@GOTPCREL(%rip), %rax         // %rax -> __cpu_capabilities
+	mov     (%rax), %eax                                    // %eax = __cpu_capabilities
+	test    $(kHasSupplementalSSE3), %eax                   // __cpu_capabilities & kHasSupplementalSSE3
+    jne      L_has_ssse3
+
+	// ----------------------------------------------------------------------------------
+	// the following is added for x86_64 without SSSE3 support
+	// it is essentially a translated copy of the i386 code without SSSE3 code
+	// ----------------------------------------------------------------------------------
+
+	// input :
+	//		 adler : rdi
+	//		 sum2  : rsi
+	// 		 buf   : rdx
+	//		 len   : rcx
+
+	pushq	%rbp
+	movq	%rsp, %rbp
+	pushq	%rbx
+
+#ifdef	KERNEL			// if for kernel, save %xmm0-%xmm11
+	subq	$200, %rsp	// allocate for %xmm0-%xmm11 (192 bytes), extra 8 to align %rsp to 16-byte boundary
+	movaps	%xmm0, -32(%rbp)
+	movaps	%xmm1, -48(%rbp)
+	movaps	%xmm2, -64(%rbp)
+	movaps	%xmm3, -80(%rbp)
+	movaps	%xmm4, -96(%rbp)
+	movaps	%xmm5, -112(%rbp)
+	movaps	%xmm6, -128(%rbp)
+#endif
+
+	#define	adler	%rdi				// 16(%rbp)
+	#define	sum2	%rsi				// 24(%ebp)
+	#define	buf		%rcx				// 32(%ebp)
+	#define	len		%rbx				// 40(%ebp)
+	#define	zero	%xmm0
+	#define ones	%xmm5
+
+	movq	%rcx, len
+	movq	%rdx, buf
+
+	.macro		modulo_BASE
+	movl		$$-2146992015, %eax		// 1/BASE in Q47
+	mull		%edi					// edx:eax = adler divided by BASE in Q47
+	shrl		$$15, %edx				// edx is now the floor integer of adler and BASE
+	imull		$$BASE, %edx, %edx		// edx * BASE
+	subq		%rdx, adler				// adler -= edx*BASE
+	movl		$$-2146992015, %eax		// 1/BASE in Q47
+	mull		%esi					// edx:eax = sum2 divided by BASE in Q47
+	shrl		$$15, %edx				// edx is now the floor integer of sum2 and BASE
+	imull		$$BASE, %edx, %eax		// eax = edx * BASE
+	subq		%rax, sum2				// sum2 -= sdx*BASE
+	.endmacro
+
+	// update adler/sum2 according to a new 16-byte vector, no ssse3
+	.macro		DO16_nossse3
+    movaps      (buf), %xmm1            // 16 bytes vector
+    movaps      %xmm1, %xmm3            // a copy of the vector, the lower 8 bytes to be shuffled into 8 words
+    movaps      %xmm1, %xmm2            // a copy of the vector, the higher 8 bytes to be shuffled into 8 words
+    psrldq      $$8, %xmm2              // shift down 8 bytes, to reuse the shuffle vector
+    punpcklbw   zero, %xmm3             // convert lower 8 bytes into 8 words
+    punpcklbw   zero, %xmm2             // convert higher 8 bytes into 8 words
+    pmullw      %xmm6, %xmm3            // lower 8 words * 16:9
+    pmullw      %xmm4, %xmm2            // higher 8 words * 8:1
+    add	        $$16, buf               // buf -> next vector
+    psadbw      zero, %xmm1             // 2 16-bit words to be added for adler in xmm1
+    paddw       %xmm2, %xmm3            // 8 16-bit words to be added for sum2 in xmm3
+    imulq       $$16, adler, %rdx       // edx = 16*adler;
+    movhlps     %xmm1, %xmm2            // higher 16-bit word (for adler) in xmm2   
+    pmaddwd     ones, %xmm3             // 4 32-bit elements to be added for sum2 in xmm3
+    paddq       %xmm2, %xmm1            // xmm1 lower 32-bit to be added to adler
+    add         %rdx, sum2              // sum2 += adler*16;
+    movhlps     %xmm3, %xmm2            // 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+    movd        %xmm1, %edx             // to be added to adler
+    paddd       %xmm2, %xmm3            // 2 32-bits elements in xmm3 to be added to sum2
+    addq        %rdx, adler             // update adler
+    movd        %xmm3, %edx             // to be added to sum2
+    psrlq       $$32, %xmm3             // another 32-bit to be added to sum2
+    addq        %rdx, sum2              // sum2 += 1st half of update
+    movd        %xmm3, %edx             // to be added to sum2
+    addq        %rdx, sum2              // sum2 += 2nd half of update
+	.endm
+
+	// need to fill up xmm4/xmm5/xmm6 only if len>=16
+	cmpq	$16, len
+	jl		0f
+
+	// set up table starting address to %eax
+	leaq    sum2_coefficients_nossse3(%rip), %rax
+
+	// reading coefficients
+	pxor	zero, zero
+	movaps  (%rax), %xmm6           // coefficients for computing sum2 : pmaddubsw 16:9
+    movaps  16(%rax), %xmm4         // coefficients for computing sum2 : pmaddubsw 8:1
+    movaps  32(%rax), ones          // coefficients for computing sum2 : pmaddwd 1,1,...,1
+0:
+
+	cmp		$NMAX, len				// len vs NMAX
+	jl		3f						// if (len < NMAX), skip the following NMAX batches processing
+
+0:									// while (len>=NMAX) {
+
+	sub		$NMAX, len				// 		len -= NMAX
+	mov		$(NMAX/16), %eax		// 		n = NMAX/16
+
+1:									// 		do {
+	DO16_nossse3					//			update adler/sum2 for a 16-byte input
+	decl 	%eax					// 			n--;
+	jg		1b						//  	} while (n);
+
+	modulo_BASE						// 		(adler/sum2) modulo BASE;
+
+	cmp		$NMAX, len				//  
+	jge		0b						// }	/* len>=NMAX */
+
+3:
+
+	sub		$16, len				// pre-decrement len by 16
+	jl		2f						// if len < 16, skip the 16-vector code
+	DO16_nossse3					// update adler/sum2 for a 16-byte input
+	sub		$16, len				// len -= 16;
+
+2:
+	add		$16, len				// post-increment len by 16
+	jz		1f						// if len==0, branch over scalar processing
+
+0:									// while (len) {
+	movzbq	(buf), %rdx				// 	new input byte
+	incq	buf						// 	buf++
+	addq	%rdx, adler				// 	adler += *buf
+	addq	adler, sum2				// 	sum2 += adler
+	decq	len						// 	len--
+	jg		0b						// }
+
+1:
+
+	modulo_BASE						// (adler/sum2) modulo BASE;
+
+	// construct 32-bit (sum2<<16 | adler) to be returned
+
+	salq	$16, sum2				// sum2 <<16
+	movq	adler, %rax				// adler		
+	orq		sum2, %rax				// sum2<<16 | adler
+
+#ifdef	KERNEL 					// if this is for kernel code, need to restore xmm registers
+	movaps	-32(%rbp), %xmm0
+	movaps	-48(%rbp), %xmm1
+	movaps	-64(%rbp), %xmm2
+	movaps	-80(%rbp), %xmm3
+	movaps	-96(%rbp), %xmm4
+	movaps	-112(%rbp), %xmm5
+	movaps	-128(%rbp), %xmm6
+	addq	$200, %rsp	// we've already restored %xmm0-%xmm11 from stack
+#endif
+
+	popq   %rbx
+	leave
+	ret
+
+
+
+	.const
+	.align	4
+sum2_coefficients_nossse3:	// used for vectorizing adler32 computation
+
+	// data for without ssse3
+
+	.word   16
+    .word   15
+    .word   14
+    .word   13
+    .word   12
+    .word   11
+    .word   10
+    .word   9
+    .word   8
+    .word   7
+    .word   6
+    .word   5
+    .word   4
+    .word   3
+    .word   2
+    .word   1
+
+	// coefficients for pmaddwd, to combine into 4 32-bit elements for sum2
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+
+
+	.text
+
+	// ----------------------------------------------------------------------------------
+	// the following is the original x86_64 adler32_vec code that uses SSSE3 instructions
+	// ----------------------------------------------------------------------------------
+
+L_has_ssse3:
+
+	// input :
+	//		 adler : rdi
+	//		 sum2  : rsi
+	// 		 buf   : rdx
+	//		 len   : rcx
+
+	pushq	%rbp
+	movq	%rsp, %rbp
+	pushq	%rbx
+
+#ifdef	KERNEL			// if for kernel, save %xmm0-%xmm11
+	subq	$200, %rsp	// allocate for %xmm0-%xmm11 (192 bytes), extra 8 to align %rsp to 16-byte boundary
+	movaps	%xmm0, -32(%rbp)
+	movaps	%xmm1, -48(%rbp)
+	movaps	%xmm2, -64(%rbp)
+	movaps	%xmm3, -80(%rbp)
+	movaps	%xmm4, -96(%rbp)
+	movaps	%xmm5, -112(%rbp)
+	movaps	%xmm6, -128(%rbp)
+	movaps	%xmm7, -144(%rbp)
+	movaps	%xmm8, -160(%rbp)
+	movaps	%xmm9, -176(%rbp)
+	movaps	%xmm10, -192(%rbp)
+	movaps	%xmm11, -208(%rbp)
+#endif
+
+	#define	adler	%rdi				// 16(%rbp)
+	#define	sum2	%rsi				// 24(%ebp)
+	#define	buf		%rcx				// 32(%ebp)
+	#define	len		%rbx				// 40(%ebp)
+	#define	zero	%xmm0
+	#define ones	%xmm5
+
+	movq	%rcx, len
+	movq	%rdx, buf
+
+	// update adler/sum2 according to a new 16-byte vector
+	.macro		DO16
+	movaps		(buf), %xmm1			// 16 bytes vector
+	movaps		%xmm1, %xmm3			// a copy of the vector, used for unsigned byte in the destination of pmaddubsw
+	addq		$$16, buf				// buf -> next vector
+	psadbw		zero, %xmm1				// 2 16-bit words to be added for adler in xmm1
+	pmaddubsw	%xmm4, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	imulq		$$16, adler, %rdx		// edx = 16*adler;
+	movhlps		%xmm1, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+	paddq		%xmm2, %xmm1			// xmm1 lower 32-bit to be added to adler
+	addq		%rdx, sum2				// sum2 += adler*16;
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm1, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addq		%rdx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addq		%rdx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addq		%rdx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// update adler/sum2 according to a new 32-byte vector
+	.macro		DO32
+	imulq		$$32, adler, %rdx		// edx = 32*adler
+	movaps		(buf), %xmm1			// 1st 16 bytes vector
+	movaps		16(buf), %xmm7			// 2nd 16 bytes vector
+	movaps		%xmm1, %xmm3			// a copy of 1st vector, used for unsigned byte in the destination of pmaddubsw
+	movaps		%xmm7, %xmm2			// a copy of 2nd vector, used for unsigned byte in the destination of pmaddubsw
+	psadbw		zero, %xmm1				// 2 16-bit words to be added for adler in xmm1
+	psadbw		zero, %xmm7				// 2 16-bit words to be added for adler in xmm7
+	addq		%rdx, sum2				// sum2 += adler*32;
+	pmaddubsw	%xmm6, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	pmaddubsw	%xmm4, %xmm2			// 8 16-bit words to be added for sum2 in xmm2
+	paddd		%xmm7, %xmm1			// 2 16-bit words to be added for adler in xmm1
+	paddw		%xmm2, %xmm3			// 8 16-bit words to be added for sum2 in xmm3
+	addq		$$32, buf				// buf -> vector for next iteration
+	movhlps		%xmm1, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+	paddq		%xmm2, %xmm1			// xmm1 lower 32-bit to be added to adler
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm1, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addq		%rdx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addq		%rdx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addq		%rdx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// update adler/sum2 according to a new 48-byte vector
+
+	.macro		DO48
+	imulq		$$48, adler, %rdx		// edx = 48*adler
+
+	movaps		(buf), %xmm7			// 1st 16 bytes vector
+	movaps		16(buf), %xmm10			// 2nd 16 bytes vector
+	movaps		32(buf), %xmm11			// 3rd 16 bytes vector
+
+	movaps		%xmm7, %xmm1			// 1st vector
+	movaps		%xmm10, %xmm2			// 2nd vector
+	movaps		%xmm11, %xmm3			// 3rd vector
+
+	psadbw		zero, %xmm7				// 1st vector for adler
+	psadbw		zero, %xmm10			// 2nd vector for adler
+	psadbw		zero, %xmm11			// 3rd vector for adler
+
+	addq		%rdx, sum2				// sum2 += adler*48;
+
+	pmaddubsw	%xmm9, %xmm1			// 8 16-bit words to be added for sum2 : 1st vector
+	pmaddubsw	%xmm6, %xmm2			// 8 16-bit words to be added for sum2 : 2nd vector
+	pmaddubsw	%xmm4, %xmm3			// 8 16-bit words to be added for sum2 : 3rd vector
+
+	pmaddwd		ones, %xmm1				// 4 32-bit elements to be added for sum2 in xmm1
+	pmaddwd		ones, %xmm2				// 4 32-bit elements to be added for sum2 in xmm2
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+
+	paddd		%xmm10, %xmm7			// 2 16-bit words to be added for adler 
+	paddd		%xmm11, %xmm7			// 2 16-bit words to be added for adler
+
+	paddd		%xmm1, %xmm3			// 4 32-bit elements to be added for sum2
+	paddd		%xmm2, %xmm3			// 4 32-bit elements to be added for sum2
+
+	addq		$$48, buf				// buf -> vector for next iteration
+
+	movhlps		%xmm7, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	paddq		%xmm2, %xmm7			// xmm7 lower 32-bit to be added to adler
+
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm7, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addq		%rdx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addq		%rdx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addq		%rdx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// update adler/sum2 according to a new 64-byte vector
+	.macro		DO64
+	imulq		$$64, adler, %rdx		// edx = 64*adler
+
+	movaps		(buf), %xmm1			// 1st 16 bytes vector
+	movaps		16(buf), %xmm7			// 2nd 16 bytes vector
+	movaps		32(buf), %xmm10			// 3rd 16 bytes vector
+	movaps		48(buf), %xmm11			// 4th 16 bytes vector
+
+	movaps		%xmm1, %xmm3			// 1st vector
+	movaps		%xmm11, %xmm2			// 4th vector
+	psadbw		zero, %xmm1				// 1st vector for adler
+	psadbw		zero, %xmm11			// 4th vector for adler
+
+	addq		%rdx, sum2				// sum2 += adler*64;
+
+	pmaddubsw	%xmm8, %xmm3			// 8 16-bit words to be added for sum2 : 1st vector
+	pmaddubsw	%xmm4, %xmm2			// 8 16-bit words to be added for sum2 : 4th vector
+	pmaddwd		ones, %xmm3				// 4 32-bit elements to be added for sum2 in xmm3
+	pmaddwd		ones, %xmm2				// 4 32-bit elements to be added for sum2 in xmm2
+
+	paddd		%xmm11, %xmm1			// 2 16-bit words to be added for adler in xmm1
+	paddd		%xmm2, %xmm3			// 4 32-bit elements to be added for sum2 in xmm3 
+
+	movaps		%xmm7, %xmm2			// 2nd vector
+	movaps		%xmm10, %xmm11			// 3rd vector
+
+	psadbw		zero, %xmm7				// 2nd vector for adler
+	psadbw		zero, %xmm10			// 3rd vector for adler
+
+	pmaddubsw	%xmm9, %xmm2			// 8 16-bit words to be added for sum2 : 2nd vector
+	pmaddubsw	%xmm6, %xmm11			// 8 16-bit words to be added for sum2 : 3rd vector 
+	pmaddwd		ones, %xmm2				// 4 32-bit elements to be added for sum2 in xmm2
+	pmaddwd		ones, %xmm11			// 4 32-bit elements to be added for sum2 in xmm11
+
+	paddd		%xmm7, %xmm1			// 2 16-bit words to be added for adler in xmm1
+	paddd		%xmm10, %xmm1			// 2 16-bit words to be added for adler in xmm1
+
+	paddd		%xmm2, %xmm3			// 4 32-bit elements to be added for sum2 in xmm3
+	paddd		%xmm11, %xmm3			// 4 32-bit elements to be added for sum2 in xmm3
+
+	addq		$$64, buf				// buf -> vector for next iteration
+
+	movhlps		%xmm1, %xmm2			// higher 16-bit word (for adler) in xmm2 	
+	paddq		%xmm2, %xmm1			// xmm1 lower 32-bit to be added to adler
+	movhlps		%xmm3, %xmm2			// 2 higher 32-bit elements of xmm3 to be added to lower 2 32-bit elements
+	movd		%xmm1, %edx				// to be added to adler
+	paddd		%xmm2, %xmm3			// 2 32-bits elements in xmm3 to be added to sum2
+	addq		%rdx, adler				// update adler
+	movd		%xmm3, %edx				// to be added to sum2
+	psrlq		$$32, %xmm3				// another 32-bit to be added to sum2
+	addq		%rdx, sum2				// sum2 += 1st half of update
+	movd		%xmm3, %edx				// to be added to sum2
+	addq		%rdx, sum2				// sum2 += 2nd half of update
+	.endm
+
+	// need to fill up xmm4/xmm5/xmm6 only if len>=16
+	cmpq	$16, len
+	jl		skip_loading_tables
+
+	// set up table starting address to %eax
+	leaq    sum2_coefficients(%rip), %rax
+
+	// reading coefficients
+	pxor	zero, zero
+	movaps	(%rax), %xmm8			// coefficients for computing sum2 : pmaddubsw 64:49
+	movaps	16(%rax), %xmm9			// coefficients for computing sum2 : pmaddubsw 48:33
+	movaps	32(%rax), %xmm6			// coefficients for computing sum2 : pmaddubsw 32:17
+	movaps	48(%rax), %xmm4			// coefficients for computing sum2 : pmaddubsw 16:1
+	movaps	64(%rax), ones			// coefficients for computing sum2 : pmaddwd 1,1,...,1
+
+skip_loading_tables:
+
+
+	cmpq	$NMAX, len				// len vs NMAX
+	jl		len_lessthan_NMAX		// if (len < NMAX), skip the following NMAX batches processing
+
+len_ge_NMAX_loop:					// while (len>=NMAX) {
+
+	subq	$NMAX, len				// 		len -= NMAX
+	movq	$(NMAX/64), %rax		// 		n = NMAX/64
+
+n_loop:								// 		do {
+	DO64							// 			update adler/sum2 for a 64-byte input
+	decq 	%rax					// 			n--;
+	jg		n_loop					//  	} while (n);
+
+	DO48							//		update adler/sum2 for a 48-byte input
+
+	modulo_BASE						// 		(adler/sum2) modulo BASE;
+
+	cmpq	$NMAX, len				//  
+	jge		len_ge_NMAX_loop		// }	/* len>=NMAX */
+
+len_lessthan_NMAX:
+
+	subq	$64, len				// pre-decrement len by 64
+	jl		len_lessthan_64			// if len < 64, skip the 64-vector code
+len64_loop:							// while (len>=64) {
+	DO64							//   update adler/sum2 for a 64-byte input
+	subq	$64, len				//   len -= 64;
+	jge		len64_loop				// } 
+
+len_lessthan_64:
+	addq	$(64-32), len			// post-increment 64 + pre-decrement 32 of len
+	jl		len_lessthan_32			// if len < 32, skip the 32-vector code
+	DO32							//   update adler/sum2 for a 32-byte input
+	subq	$32, len				//   len -= 32;
+
+len_lessthan_32:
+
+	addq	$(32-16), len			// post-increment by 32 + pre-decrement by 16 on len
+	jl		len_lessthan_16			// if len < 16, skip the 16-vector code
+	DO16							// update adler/sum2 for a 16-byte input
+	subq	$16, len				// len -= 16;
+
+len_lessthan_16:
+	addq	$16, len				// post-increment len by 16
+	jz		len_is_zero				// if len==0, branch over scalar processing
+
+scalar_loop:						// while (len) {
+	movzbq	(buf), %rdx				// 	new input byte
+	incq	buf						// 	buf++
+	addq	%rdx, adler				// 	adler += *buf
+	addq	adler, sum2				// 	sum2 += adler
+	decq	len						// 	len--
+	jg		scalar_loop				// }
+
+len_is_zero:
+
+	modulo_BASE						// (adler/sum2) modulo BASE;
+
+	// construct 32-bit (sum2<<16 | adler) to be returned
+
+	salq	$16, sum2				// sum2 <<16
+	movq	adler, %rax				// adler		
+	orq		sum2, %rax				// sum2<<16 | adler
+
+
+#ifdef	KERNEL			// if for kernel, restore %xmm0-%xmm11
+	movaps	-32(%rbp), %xmm0
+	movaps	-48(%rbp), %xmm1
+	movaps	-64(%rbp), %xmm2
+	movaps	-80(%rbp), %xmm3
+	movaps	-96(%rbp), %xmm4
+	movaps	-112(%rbp), %xmm5
+	movaps	-128(%rbp), %xmm6
+	movaps	-144(%rbp), %xmm7
+	movaps	-160(%rbp), %xmm8
+	movaps	-176(%rbp), %xmm9
+	movaps	-192(%rbp), %xmm10
+	movaps	-208(%rbp), %xmm11
+	addq	$200, %rsp	// we've already restored %xmm0-%xmm11 from stack
+#endif
+
+	popq   %rbx
+	leave							// pop ebp out from stack
+	ret
+
+
+	.const
+	.align	4
+sum2_coefficients:	// used for vectorizing adler32 computation
+
+	// coefficients for pmaddubsw instruction, used to generate 16-bit elements for sum2
+
+	.byte	64
+	.byte	63
+	.byte	62
+	.byte	61
+	.byte	60
+	.byte	59
+	.byte	58
+	.byte	57
+	.byte	56
+	.byte	55
+	.byte	54
+	.byte	53
+	.byte	52
+	.byte	51
+	.byte	50
+	.byte	49
+	.byte	48
+	.byte	47
+	.byte	46
+	.byte	45
+	.byte	44
+	.byte	43
+	.byte	42
+	.byte	41
+	.byte	40
+	.byte	39
+	.byte	38
+	.byte	37
+	.byte	36
+	.byte	35
+	.byte	34
+	.byte	33
+	.byte	32
+	.byte	31
+	.byte	30
+	.byte	29
+	.byte	28
+	.byte	27
+	.byte	26
+	.byte	25
+	.byte	24
+	.byte	23
+	.byte	22
+	.byte	21
+	.byte	20
+	.byte	19
+	.byte	18
+	.byte	17
+	.byte	16
+	.byte	15
+	.byte	14
+	.byte	13
+	.byte	12
+	.byte	11
+	.byte	10
+	.byte	9
+	.byte	8
+	.byte	7
+	.byte	6
+	.byte	5
+	.byte	4
+	.byte	3
+	.byte	2
+	.byte	1
+
+	// coefficients for pmaddwd, to combine into 4 32-bit elements for sum2
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+	.word	1
+
+#endif	// (defined __i386__)
+
+#endif	// (defined __i386__ || defined __x86_64__)
+
+#endif // defined VEC_OPTIMIZE
--- a/AddOn/ZAssembly/crc32lt_arm64.s	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZAssembly/crc32lt_arm64.s	1695412879.000000000
@@ -0,0 +1,261 @@
+#if defined(VEC_OPTIMIZE) && defined(__arm64__)
+/*
+	This arm64 assembly code implements crc32_little_aligned_vector that computes the CRC32 of an initial crc 
+	and a 16-byte aligned byte sequence. 
+
+	uint32_t	crc32_little_aligned_vector(uint32_t crc, unsigned char *input, int	len);
+
+	This function SHOULD NOT be called directly. It should be called in a wrapper
+	function (such as crc32_little in crc32.c) that 1st align an input buffer to 16-byte (update crc along the way),
+	and make sure that len is at least 16 and SHOULD be a multiple of 16.
+
+	input :
+		uint32_t	crc;		// input crc
+		uint8_t		*input;		// input byte sequence, this one MUST be 16-byte aligned in the caller
+		int			len;		// len MUST be no less than 16, and is a multiple of 16
+
+	output :
+		the final 32-bit crc
+
+	The implementation here is based on an Intel white paper
+
+	"White Paper: Fast CRC Computation for Generic Polynomials Using PCLMULQDQ Instruction"
+
+	http://download.intel.com/design/intarch/papers/323102.pdf
+
+	The implementation here implements the bit-reflected version of CRC32, 
+	for which the generator polynomial is 0x1DB710641 (in bit-reflected manner).
+
+	1. The input crc is xored with the 1st 4 bytes of the input word.
+
+	2. let m(x) = D(x)*x^T + G(x), m(x) mod P(x) = [D(x) * (x^T mod P(x)) + G(x) ] mod P(x)
+		this means a vector D(x) that is scaled up x^T can be multiplied with precomputed (x^T mod P(x)),
+		and then be added to G(x). This will be congruent to the result m(x) mod P(x). This is called folding a
+		vector.
+
+		let there be at least 8 128-bit vectors v0, v1, v2, v3, v4, v5, v6, v7
+		we can apply the folding to (v0,x4), (v1,v5), (v2,v6), (v3,v7), respectively, with T = 512
+		for example, v4' = v0 * (x^512 mod P(x)) + v4;
+		this effectively shortens the sequence by 4 vectors, producing a congruent sequence v4', v5', v6', v7'
+		this is called folding by 4
+
+	3. after len <= 7*16, we keep doing folding by 1 until we are down to 128-bit.
+			v1' = v0 * (x^128 mod P(x)) + v1; 
+
+
+		regarding the folding operation above, v0 is 128-bit and (x^T mod P(x)) is 32-bit.
+		We can write v0 = H(x) ^ X^64 + L(x). and the 128-bit * 32-bit multiply  operation becomes
+			H(x) * (x^(T+64) mod P(x)) + L(x) * (x^T mod P(x)
+
+		we can then use pclmulqdq instruction twice and xor them to get the result of v0 * (x^T mod P(x)) 
+
+	4 when we are down to 128-bit v0, we need to append 32-bit zero to the end to compute the final crc
+		
+		a. v0 = (H(x)*x^96 + L(x)*x^32, a P(x) congruent vector is v1 = H(x) * (x^96 mod P(x)) + L(x) * x^32, 
+			which is 96-bit
+		b. v1 = H'(x)*x^64 + L'(x), a P(x) congruen vector is v2 = H'(x) * (x^64 mod P(x)) + L'(x), which is 64-bit.
+		c. use Barrett Reduction to divide 64-bit v2 by P(x) into the final 32-bit crc.
+		 	  i. T1 = floor(v2/x^32) * u;     // u is actually 1/P(x)
+			 ii. T2 = floor(T1/x^32) * P(x);
+			iii. crc = (v2 + T2) mod x^32;
+ 
+*/
+
+	.text
+	.align 4,0x90
+.globl _crc32_little_aligned_vector
+_crc32_little_aligned_vector:
+
+	// input :
+	//		 crc : w0
+	//		 buf : x1
+	// 		 len : w2
+
+#if KERNEL
+    sub     x3, sp, #12*16   
+    sub     sp, sp, #12*16   
+    st1.4s  {v0,v1,v2,v3},[x3],#4*16
+    st1.4s  {v4,v5,v6,v7},[x3],#4*16
+    st1.4s  {v16,v17,v18,v19},[x3],#4*16
+#endif
+
+	#define	crc		w0
+	#define	buf		x1
+	#define	len		w2
+	#define	t		x3
+
+	#define	K12		v16
+	#define	K34		v17
+	#define	K56		v18
+	#define	uPx		v19
+
+	adrp	t, L_coefficients@PAGE
+	eor.16b	v0, v0, v0
+	ld1.4s	{v1}, [buf], #16					// 1st vector
+	add		t, t, L_coefficients@PAGEOFF		// t points to constants table
+	ins		v0.s[0], crc						// v0 = initial crc
+	ld1.4s	{K12,K34,K56,uPx}, [t]				// store parameters in v16-v19
+	eor.16b	v0, v0, v1							// v0 = initial crc xored with 1st vector	
+
+	/* if only one vector, we are down to final 128-bit vector */
+	subs	len, len, #16
+	b.le 	L_128bits
+
+	/* check if there are at least 3 more vectors */
+	cmp		len, #48
+	b.lt	L_no_more_4_vectors
+
+	/* read the next 3 vectors */
+	ld1.4s	{v1,v2,v3}, [buf], #48
+
+	/* adjust len (-48), and pre-decrement len by 64 to check whether there are at least 4 more vectors */
+	subs	len, len, #(48+64)
+	b.lt	L_foldv13
+
+L_FOLD_BY_4:	
+
+/*	
+	.macro	FOLD4
+	ld1.4s	{v4}, [buf], #16			// new vector	
+	pmull.1q	v6, $0, K12				// H(x) * {x^[512+64] mod P(x)}
+	pmull2.1q	$1, $0, K12				// L(x) * {x^[512] mod P(x)}
+	eor.16b		v4, v4, v6				// H(x) * {x^[512+64] mod P(x)} xor with the new vector with offset of 512 bits
+	eor.16b		$0, $0, v4				// xor with L(x) * {x^[512] mod P(x)}
+	.endm
+
+	FOLD4	v0, v0
+	FOLD4	v1, v1
+	FOLD4	v2, v2
+	FOLD4	v3, v3
+
+	the above code snippet is unrolled to give better performance 
+*/
+
+	ld1.4s	{v4,v5}, [buf], #32
+    mov.16b     v6, v4
+	pmull.1q	v4, v0, K12
+	eor.16b		v4, v4, v6
+
+    mov.16b     v7, v5
+	pmull.1q	v5, v1, K12
+	eor.16b		v5, v5, v7
+
+	pmull2.1q	v0, v0, K12
+	eor.16b		v0, v0, v4
+
+	pmull2.1q	v1, v1, K12
+	eor.16b		v1, v1, v5
+
+	ld1.4s	{v4,v5}, [buf], #32
+    mov.16b     v6, v4
+	pmull.1q	v4, v2, K12
+	eor.16b		v4, v4, v6
+
+    mov.16b     v7, v5
+	pmull.1q	v5, v3, K12
+	eor.16b		v5, v5, v7
+
+	pmull2.1q	v2, v2, K12
+	eor.16b		v2, v2, v4
+
+	pmull2.1q	v3, v3, K12
+	eor.16b		v3, v3, v5
+
+	/* decrement len by 64, repeat the loop if len>0 */
+	subs     	len, len, #64
+	b.gt		L_FOLD_BY_4
+
+
+	.macro	FOLD1
+	pmull.1q	v4, v0, K34			// H(x) * {x^[128+64] mod P(x)}
+	eor.16b		v4, v4, $0			// H(x) * {x^[128+64] mod P(x)} xor with the new vector
+	pmull2.1q	v0, v0, K34			// L(x) * {x^128 mod P(x)}
+	eor.16b		v0, v0, v4			// xor with L(x) * {x^128 mod P(x)}
+	.endm
+
+	/* FOLD1 of v1-v3 into v0 */
+L_foldv13:
+	FOLD1	v1
+	FOLD1	v2
+	FOLD1	v3
+
+	/* post-increment len by 64 */
+	add		len, len, #64
+
+L_no_more_4_vectors:
+
+	/* pre-decrement len by 16 to detect whether there is still some vector to process */
+	subs		len, len, #16
+	b.lt 		L_128bits
+L_FOLD_BY_1:	
+	ld1.4s		{v1}, [buf], #16
+	FOLD1		v1				/* folding into the new vector */
+	subs		len, len, #16
+	b.ge		L_FOLD_BY_1		/* until there is no more vector */
+
+L_128bits:		/* we've arrived at the final 128-bit vector */
+
+    /* reduction from 128-bits to 64-bits */
+	eor.16b		v2, v2, v2
+	eor.16b		v3, v3, v3
+	pmull.1q	v1, v0, K56 
+	ins			v2.d[0], v0.d[1]
+	eor.16b		v0, v2, v1 
+	ins			v3.s[2], v0.s[0]
+	ins			v0.s[0], v0.s[1]
+	ins			v0.s[1], v0.s[2]
+
+    mov.16b     v1, v0
+	pmull2.1q	v0, v3, K56
+	eor.16b		v0, v0, v1 
+	
+	/* 
+        barrett reduction:
+
+            T1 = floor(R(x)/x^32) * [1/P(x)];   R/P
+            T2 = floor(T1/x^32) * P(x);         int(R/P)*P;
+            CRC = (R+int(R/P)*P) mod x^32;      R-int(R/P)*P
+            
+    */
+	ins			v3.s[0], v0.s[0]
+	pmull.1q	v1, v3, uPx
+	ins			v3.s[2], v1.s[0]
+
+    mov.16b     v1, v0
+	pmull2.1q	v0, v3, uPx
+	eor.16b		v0, v0, v1
+
+	fmov		x0, d0
+	lsr			x0, x0, #32	
+	
+#if KERNEL
+    ld1.4s  {v0,v1,v2,v3},[sp],#4*16
+    ld1.4s  {v4,v5,v6,v7},[sp],#4*16
+    ld1.4s  {v16,v17,v18,v19},[sp],#4*16
+#endif
+	ret		lr
+
+	.const
+	.align	4
+L_coefficients:	// used for vectorizing crc32 computation using pmull
+
+#define	K1	0x154442bd4
+#define	K2	0x1c6e41596
+#define	K3	0x1751997d0
+#define	K4	0x0ccaa009e
+#define	K5	0x0ccaa009e		// x^96 mod	P(x)
+#define	K6	0x163cd6124		// x^64 mod P(x)
+#define	ux	0x1F7011641
+#define	Px	0x1DB710641
+
+	.quad	K1
+	.quad	K2
+	.quad	K3
+	.quad	K4
+	.quad	K5
+	.quad	K6
+	.quad	ux
+	.quad	Px	
+
+
+#endif // defined VEC_OPTIMIZE
--- a/AddOn/ZAssembly/crc32lt_intel.s	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZAssembly/crc32lt_intel.s	1695412879.000000000
@@ -0,0 +1,290 @@
+#if defined(VEC_OPTIMIZE) && defined(__x86_64__)
+/*
+	This x86_64 assembly code implements crc32_little_aligned_vector that computes the CRC32 of an initial crc 
+	and a 16-byte aligned byte sequence. 
+
+	uint32_t	crc32_little_aligned_vector(uint32_t crc, unsigned char *input, int	len);
+
+	This function SHOULD NOT be called directly. It should be called in a wrapper
+	function (such as crc32_little in crc32.c) that 1st align an input buffer to 16-byte (update crc along the way),
+	and make sure that len is at least 16 and SHOULD be a multiple of 16.
+
+	input :
+		uint32_t	crc;		// input crc
+		uint8_t		*input;		// input byte sequence, this one MUST be 16-byte aligned in the caller
+		int			len;		// len MUST be no less than 16, and is a multiple of 16
+
+	output :
+		the final 32-bit crc
+
+	The implementation here is based on an Intel white paper
+
+	"White Paper: Fast CRC Computation for Generic Polynomials Using PCLMULQDQ Instruction"
+
+	http://download.intel.com/design/intarch/papers/323102.pdf
+
+	The implementation here implements the bit-reflected version of CRC32, 
+	for which the generator polynomial is 0x1DB710641 (in bit-reflected manner).
+
+	1. The input crc is xored with the 1st 4 bytes of the input word.
+
+	2. let m(x) = D(x)*x^T + G(x), m(x) mod P(x) = [D(x) * (x^T mod P(x)) + G(x) ] mod P(x)
+		this means a vector D(x) that is scaled up x^T can be multiplied with precomputed (x^T mod P(x)),
+		and then be added to G(x). This will be congruent to the result m(x) mod P(x). This is called folding a
+		vector.
+
+		let there be at least 8 128-bit vectors v0, v1, v2, v3, v4, v5, v6, v7
+		we can apply the folding to (v0,x4), (v1,v5), (v2,v6), (v3,v7), respectively, with T = 512
+		for example, v4' = v0 * (x^512 mod P(x)) + v4;
+		this effectively shortens the sequence by 4 vectors, producing a congruent sequence v4', v5', v6', v7'
+		this is called folding by 4
+
+	3. after len <= 7*16, we keep doing folding by 1 until we are down to 128-bit.
+			v1' = v0 * (x^128 mod P(x)) + v1; 
+
+
+		regarding the folding operation above, v0 is 128-bit and (x^T mod P(x)) is 32-bit.
+		We can write v0 = H(x) ^ X^64 + L(x). and the 128-bit * 32-bit multiply  operation becomes
+			H(x) * (x^(T+64) mod P(x)) + L(x) * (x^T mod P(x)
+
+		we can then use pclmulqdq instruction twice and xor them to get the result of v0 * (x^T mod P(x)) 
+
+	4 when we are down to 128-bit v0, we need to append 32-bit zero to the end to compute the final crc
+		
+		a. v0 = (H(x)*x^96 + L(x)*x^32, a P(x) congruent vector is v1 = H(x) * (x^96 mod P(x)) + L(x) * x^32, 
+			which is 96-bit
+		b. v1 = H'(x)*x^64 + L'(x), a P(x) congruen vector is v2 = H'(x) * (x^64 mod P(x)) + L'(x), which is 64-bit.
+		c. use Barrett Reduction to divide 64-bit v2 by P(x) into the final 32-bit crc.
+		 	  i. T1 = floor(v2/x^32) * u;     // u is actually 1/P(x)
+			 ii. T2 = floor(T1/x^32) * P(x);
+			iii. crc = (v2 + T2) mod x^32;
+ 
+*/
+
+
+	.text
+	.align 4,0x90
+.globl _crc32_little_aligned_vector
+_crc32_little_aligned_vector:
+
+	// input :
+	//		 crc : edi
+	//		 buf : rsi
+	// 		 len : rdx
+
+	// symbolizing x86_64 registers
+
+	#define	crc		%edi
+	#define	buf		%rsi
+	#define	len		%rdx
+	#define	tab		%rcx
+
+	#define	v0		%xmm0
+	#define	v1		%xmm1
+	#define	v2		%xmm2
+	#define	v3		%xmm3
+	#define	v4		%xmm4
+	#define	v5		%xmm5
+
+	// push rbp, sp should now be 16-byte aligned
+	pushq	%rbp
+	movq	%rsp, %rbp
+
+#ifdef	KERNEL
+	/* 
+		allocate 6*16 = 96 stack space and save %xmm0-%xmm7
+	*/
+	subq	$96, %rsp
+	movaps	v0, -16(%rbp)
+	movaps	v1, -32(%rbp)
+	movaps	v2, -48(%rbp)
+	movaps	v3, -64(%rbp)
+	movaps	v4, -80(%rbp)
+	movaps	v5, -96(%rbp)
+#endif
+
+	/*
+		set up the table pointer and use 16-byte data directly in pclmulqdq
+		tried movaps to %xmm7, and use %xmm7, performance about the same
+	*/
+	leaq    L_coefficients(%rip), tab
+	#define	K12		(tab)
+	#define	K34		16(tab)
+	#define	K56		32(tab)
+	#define	uPx		48(tab)
+
+	/* load the initial crc and xor with the 1st 16-byte vector */
+	movd	crc, v0
+	pxor	(buf), v0
+
+	/* if this is the only vector, we've achieve the final 128-bit vector */ 
+	add		$16, buf
+	sub		$16, len
+	jle		L_128bits
+
+	/* make sure there are at least 3 more vectors */
+	cmp		$48, len
+	jl		L_no_more_4_vectors
+
+	/* read the next 3 vectors*/
+	movdqa	(buf), v1	
+	movdqa	16(buf), v2	
+	movdqa	32(buf), v3	
+	add		$48, buf
+
+	/* pre-decrement len by 64, to check whether there are at least 4 more vectors */
+	sub		$48+64, len
+	jl		L_foldv13
+
+	/*	-------------------------------------------------
+		the main loop, folding 4 vectors per iterations
+		------------------------------------------------- 
+	*/
+L_FOLD_BY_4:	
+
+/*
+	.macro	FOLD4
+	movdqa		$0, v4					// a copy of H(x)x^64 + L(x)
+	pclmulqdq	$$0x00, K12, $0			// H(x) * {x^[512+64] mod P(x)}
+	pclmulqdq	$$0x11, K12, v4			// L(x) * {x^[512] mod P(x)}
+	pxor		$1, $0					// H(x) * {x^[512+64] mod P(x)} xor with the new vector with offset of 512 bits
+	pxor		v4, $0					// xor with L(x) * {x^[512] mod P(x)}
+	.endm
+
+	FOLD4	v0, 0(buf)
+	FOLD4	v1, 16(buf)
+	FOLD4	v2, 32(buf)
+	FOLD4	v3, 48(buf)
+
+	the above code snippet is unfolded to save ~ 0.02 cycle/byte
+
+*/
+
+	movdqa		v0, v4
+	movdqa		v1, v5
+	pclmulqdq	$0x00, K12, v0
+	pclmulqdq	$0x00, K12, v1
+	pclmulqdq	$0x11, K12, v4
+	pclmulqdq	$0x11, K12, v5
+	pxor		0(buf), v0
+	pxor		16(buf), v1
+	pxor		v4, v0
+	pxor		v5, v1
+	movdqa		v2, v4
+	movdqa		v3, v5
+	pclmulqdq	$0x00, K12, v2
+	pclmulqdq	$0x00, K12, v3
+	pclmulqdq	$0x11, K12, v4
+	pclmulqdq	$0x11, K12, v5
+	pxor		32(buf), v2
+	pxor		48(buf), v3
+	pxor		v4, v2
+	pxor		v5, v3
+
+	add			$64, buf
+	sub     	$64, len
+	ja			L_FOLD_BY_4
+
+
+	/*
+		now sequentially fold v0 into v1,v2,v3
+	*/
+L_foldv13:
+
+	.macro	FOLD1
+	movdqa		v0, v4				// a copy of v0 = H(x)x^64 + L(x)
+	pclmulqdq	$$0x00, K34, v0		// H(x) * {x^[128+64] mod P(x)}
+	pclmulqdq	$$0x11, K34, v4		// L(x) * {x^128 mod P(x)}
+	pxor		$0, v0				// H(x) * {x^[128+64] mod P(x)} xor with the new vector v1/v2/v3
+	pxor		v4, v0				// xor with L(x) * {x^128 mod P(x)}
+	.endm
+
+	/* FOLD1 of v1-v3 into v0 */
+	FOLD1	v1
+	FOLD1	v2
+	FOLD1	v3
+
+	/* post-increment len by 64 */
+	add		$64, len
+
+L_no_more_4_vectors:
+
+	/* pre-decrement len by 16 to detect whether there is still some vector to process */
+	sub			$16, len
+	jl			L_128bits
+L_FOLD_BY_1:	
+	FOLD1		(buf)			/* folding into the new vector */
+	add			$16, buf
+	sub			$16, len
+	jae			L_FOLD_BY_1		/* until no more new vector */
+
+L_128bits:		/* we've arrived at the final 128-bit vector */
+
+	/* reduction from 128-bits to 64-bits */
+	movdqa		v0, v1
+	pclmulqdq	$0x00, K56, v1		// v1 = H(x) * K5 96-bits
+	psrldq		$8, v0				// v0 = L(x) 64-bits
+	pxor		v2, v2
+	pxor		v1, v0
+	movss		v0, v2
+	psrldq		$4, v0				// low 64-bit
+	pclmulqdq	$0x10, K56, v2
+	pxor		v2, v0
+
+	/* 
+		barrett reduction:
+
+			T1 = floor(R(x)/x^32) * [1/P(x)];	R/P
+			T2 = floor(T1/x^32) * P(x);			int(R/P)*P;
+			CRC = (R+int(R/P)*P) mod x^32;		R-int(R/P)*P
+			
+	*/
+	pxor		v1, v1
+	pxor		v2, v2
+	movss		v0, v1
+	pclmulqdq	$0x00, uPx, v1		// T1 = floor(R/x^32)*u
+	movss		v1, v2
+	pclmulqdq	$0x10, uPx, v2		// T2 = floor(T1/x^32)*P
+	pxor		v2, v0
+	psrldq		$4, v0				// low 64-bit
+	movd		v0, %eax
+
+
+#ifdef	KERNEL
+	// restore xmm0-xmm7, and deallocate 96 bytes from stack 
+	movaps	-16(%rbp), v0
+	movaps	-32(%rbp), v1
+	movaps	-48(%rbp), v2
+	movaps	-64(%rbp), v3
+	movaps	-80(%rbp), v4
+	movaps	-96(%rbp), v5
+	addq	$96, %rsp
+#endif
+
+	leave
+	ret
+
+	.const
+	.align	4
+L_coefficients: // used for vectorizing crc32 computation using pclmulqdq 
+
+#define	K1	0x154442bd4
+#define	K2	0x1c6e41596
+#define	K3	0x1751997d0
+#define	K4	0x0ccaa009e
+#define	K5	0x0ccaa009e		// x^96 mod	P(x)
+#define	K6	0x163cd6124		// x^64 mod P(x)
+#define	ux	0x1F7011641
+#define	Px	0x1DB710641
+
+	.quad	K1
+	.quad	K2
+	.quad	K3
+	.quad	K4
+	.quad	K5
+	.quad	K6
+	.quad	ux
+	.quad	Px	
+
+
+#endif // VEC_OPTIMIZE && __x86_64__
--- a/AddOn/ZTests/ztest_common.c	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZTests/ztest_common.c	1695412879.000000000
@@ -0,0 +1,280 @@
+// ZLIB tests tools
+// CM 2022/10/18
+#include "ztest_common.h"
+
+#pragma mark - BENCHMARKING
+
+#include <kperf/kpc.h>
+
+static int kKPC_OK = 0; // Did we init properly?
+
+// Init counters. Return 0 on success, -1 on failure.
+int kpc_cycles_init(void) // OK
+{
+  if (kpc_force_all_ctrs_set(1)) { perror("kpc_force_all_ctrs_set"); return -1; }
+  
+  // Enable only fixed counters
+  if (kpc_set_counting(KPC_CLASS_FIXED_MASK)) { perror("kpc_set_counting"); return -1; }
+  if (kpc_set_thread_counting(KPC_CLASS_FIXED_MASK)) { perror("kpc_set_thread_counting"); return -1; }
+  
+  kKPC_OK = 1; // OK
+  return 0;
+}
+
+// Get cycles
+/*
+ ARM
+ PMC0 is always CORE_CYCLE
+ PMC1 is always number of retired instructions
+ 
+ INTEL
+ 0 is retired instructions
+ 1 is cycles
+ 2 is TSC
+ */
+uint64_t kpc_get_cycles(void) // OK
+{
+  static uint64_t fallback = 0;
+  uint64_t counters[32];
+  
+  if (!kKPC_OK) return fallback++; // Init failed, return pseudo value
+  
+  kpc_get_thread_counters(0, 32, counters);
+#if defined(__x86_64__) || defined(__i386__)
+  return counters[1]; // intel
+#else
+  return counters[0]; // arm
+#endif
+}
+
+#pragma mark - BUFFER API
+
+// Encode buffer using zlib. Return number of compressed bytes, 0 on failure.
+size_t zlib_encode_buffer(uint8_t* dst_buffer, size_t dst_size,
+                          uint8_t* src_buffer, size_t src_size, int level, int rfc1950, int fixed) // OK
+{
+  z_stream z;
+  int ok = 1;
+  
+  // Handle edge cases
+  if (dst_size > UINT32_MAX) dst_size = UINT32_MAX;
+  if (src_size > UINT32_MAX) return 0;
+  
+  // Setup stream
+  bzero(&z, sizeof(z_stream));
+  z.avail_in  = (uint32_t)src_size;
+  z.avail_out = (uint32_t)dst_size;
+  z.next_in  = src_buffer;
+  z.next_out = dst_buffer;
+  
+  // Encode buffer
+  if (deflateInit2(&z, level, Z_DEFLATED,
+                   rfc1950 ? 15 : -15, 8,
+                   fixed ? Z_FIXED : Z_DEFAULT_STRATEGY) != Z_OK) return 0; // Failed
+  if (deflate(&z, Z_FINISH) != Z_STREAM_END) ok = 0;
+  if (deflateEnd(&z) != Z_OK) ok = 0;
+  
+  return ok ? z.total_out : 0;
+}
+
+// Decode buffer using zlib. Return number of uncompressed bytes, 0 on failure.
+// Supports truncated decodes.
+size_t zlib_decode_buffer(uint8_t* dst_buffer, size_t dst_size,
+                          uint8_t* src_buffer, size_t src_size, int rfc1950) // OK
+{
+  z_stream z;
+  int ok = 1;
+  int status;
+  
+  // Handle edge cases
+  if (dst_size > UINT32_MAX) dst_size = UINT32_MAX;
+  if (src_size > UINT32_MAX) return 0;
+  
+  // Setup stream
+  bzero(&z, sizeof(z));
+  z.avail_in  = (uint32_t)src_size;
+  z.avail_out = (uint32_t)dst_size;
+  z.next_in  = src_buffer;
+  z.next_out = dst_buffer;
+  
+  // Decode buffer
+  if (inflateInit2(&z, rfc1950 ? 15: -15) != Z_OK) return 0; // Failed
+  status = inflate(&z, Z_FINISH);
+  
+  // No success (EoS or partial decode)?
+  if (!   ((status == Z_STREAM_END) ||
+           ((status == Z_BUF_ERROR) && (z.total_out == dst_size)))) ok = 0;
+  
+  // Cleanup and return
+  if (inflateEnd(&z) != Z_OK) ok = 0;
+  return ok ? z.total_out : 0;
+}
+
+// Decode buffer using zlib. Torture streaming API. Return number of uncompressed bytes, 0 on failure.
+size_t zlib_decode_torture(uint8_t* dst_buffer, size_t dst_size,
+                           uint8_t* src_buffer, size_t src_size, int rfc1950) // OK
+{
+  z_stream z;
+  int ok = 1;
+  int status;
+  
+  // Handle edge cases
+  if (dst_size > UINT32_MAX) dst_size = UINT32_MAX;
+  if (src_size > UINT32_MAX) return 0;
+  
+  // Setup stream
+  bzero(&z, sizeof(z));
+  z.next_in = src_buffer;
+  z.next_out = dst_buffer;
+  
+  // Decode buffer
+  status = inflateInit2(&z, rfc1950 ? 15: -15);
+  if (status != Z_OK) return 0; // Failed
+  
+  // Decode step by step
+  for (;;)
+  {
+    // Use small random input/output buffers
+    z.avail_in  = (uint32_t)(src_size - z.total_in);
+    z.avail_out = (uint32_t)(dst_size - z.total_out);
+    z.avail_in  %= (rand() % 1000) + 1;
+    z.avail_out %= (rand() % 1000) + 1;
+    
+    // Decode
+    status = inflate(&z, Z_NO_FLUSH);
+    
+    // Continue?
+    if (status == Z_OK) continue;
+    
+    // Are we done?
+    if (status == Z_STREAM_END) break;
+    
+    // One more try?
+    if ((status == Z_BUF_ERROR) &&
+        ((src_size - z.total_in > 0) || (dst_size - z.total_out >= 258))) continue;
+    
+    // Something wrong!
+    {
+      PLOG("status = %d, msg = %s\n", status, z.msg);
+      ok = 0;
+      break;
+    }
+  }
+  if (inflateEnd(&z) != Z_OK) ok = 0;
+  
+  return ok ? z.total_out : 0;
+}
+
+#pragma mark - INFBACK API
+
+typedef struct
+{
+  uint8_t* src_buffer;
+  uint8_t* dst_buffer;
+  size_t src_avail;
+  size_t dst_avail;
+} s_infback;
+
+// Callback to read data. Return number of bytes available.
+static unsigned infback_read(void* arg, z_const unsigned char** data) // OK
+{
+  s_infback* s = arg;
+  size_t k = s->src_avail;
+
+  // Setup data
+  *data = s->src_buffer;
+  
+  // Update state
+  if (k > 0x400) k = 0x400; // Clamp to 1K
+  s->src_buffer += k;
+  s->src_avail -= k;
+  
+  return (unsigned)k;
+}
+
+// Callback to write data. Return on success.
+static int infback_write(void* arg, unsigned char* data, unsigned size) // OK
+{
+  s_infback* s = arg;
+  int ok = 1;
+
+  // Truncate write?
+  if (size > s->dst_avail) { ok = 0; size = (unsigned)s->dst_avail; }
+  
+  // Copy data and update state
+  bcopy(data, s->dst_buffer, size);
+  s->dst_buffer += size;
+  s->dst_avail -= size;
+  
+  return ok ? 0 : -1;
+}
+
+// Decode buffer using zlib using infback interface. Return number of uncompressed bytes, 0 on failure.
+// Supports truncated decodes.
+size_t zlib_decode_infback(uint8_t* dst_buffer, size_t dst_size,
+                           uint8_t* src_buffer, size_t src_size) // OK
+{
+  z_stream z;
+  s_infback s;
+  uint8_t* window = NULL;
+  int ok = 1;
+  int status;
+  
+  // Allocate window
+  window = malloc(1 << 15);
+  if (window == NULL) { ok = 0; goto END; }
+  
+  // Setup state
+  bzero(&z, sizeof(z));
+  if (inflateBackInit(&z, 15, window) != Z_OK) { ok = 0; goto END; }
+  
+  // Call inflateBack
+  s.dst_buffer = dst_buffer; s.dst_avail = dst_size;
+  s.src_buffer = src_buffer; s.src_avail = src_size;
+  status = inflateBack(&z, infback_read, &s, infback_write, &s);
+
+  // Not successful or truncated decode?
+  if (!(status == Z_STREAM_END) &&
+      !((status == Z_BUF_ERROR) && (s.dst_avail == 0))) ok = 0;
+  
+  // Free state
+  if (inflateBackEnd(&z) != Z_OK) ok = 0;
+  
+END:
+  free(window);
+  return ok ? dst_size - s.dst_avail : 0;
+}
+
+#pragma mark - CHECKSUMS
+
+// Return Crc32 of DATA[LEN]. Naive implementation.
+uint32_t simple_crc32(uint8_t* src_buffer, const size_t src_size) // OK
+{
+  uint32_t r = 0xffffffff;
+  
+  for (size_t i = 0; i < src_size; i++)
+  {
+    r ^= src_buffer[i];
+    for (int j = 0; j < 8; j++)
+    {
+      r = (r >> 1) ^ (-(r & 1) & 0xedb88320);
+    }
+  }
+  
+  return r ^ 0xffffffff;
+}
+
+// Return Adler32 of DATA[LEN]. Naive implementation.
+uint32_t simple_adler32(const unsigned char* src, const size_t src_size) // OK
+{
+  uint32_t a = 1, b = 0;
+  
+  // Process each byte of the data in order
+  for (size_t i = 0; i < src_size; i++)
+  {
+    a = (a + src[i]) % 65521;
+    b = (b + a) % 65521;
+  }
+  
+  return (b << 16) | a;
+}
--- a/AddOn/ZTests/ztest_common.h	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZTests/ztest_common.h	1695412879.000000000
@@ -0,0 +1,45 @@
+// ZLIB tests tools
+// CM 2022/08/29
+#pragma once
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <memory.h>
+#include <assert.h>
+#include "zlib.h"
+
+#define PLOG(F, ...)  do { fprintf(stderr, F"\n", ##__VA_ARGS__); } while (0)
+#define PFAIL(F, ...) do { PLOG("[ERR "__FILE__":%s:%d] "F, __FUNCTION__, __LINE__, ##__VA_ARGS__); exit(1); } while (0)
+
+#pragma mark - BENCHMARKING
+
+int kpc_cycles_init(void);
+uint64_t kpc_get_cycles(void);
+
+#pragma mark - BUFFER API
+
+// Encode buffer using zlib. Return number of compressed bytes, 0 on failure.
+size_t zlib_encode_buffer(uint8_t* dst_buffer, size_t dst_size,
+                          uint8_t* src_buffer, size_t src_size, int level, int rfc1950, int fixed);
+
+// Decode buffer using zlib. Return number of uncompressed bytes, 0 on failure.
+// Supports truncated decodes.
+size_t zlib_decode_buffer(uint8_t* dst_buffer, size_t dst_size,
+                          uint8_t* src_buffer, size_t src_size, int rfc1950);
+
+// Decode buffer using zlib using infback interface. Return number of uncompressed bytes, 0 on failure.
+// Supports truncated decodes.
+size_t zlib_decode_infback(uint8_t* dst_buffer, size_t dst_size,
+                           uint8_t* src_buffer, size_t src_size);
+
+// Decode buffer using zlib. Torture streaming API. Return number of uncompressed bytes, 0 on failure.
+size_t zlib_decode_torture(uint8_t* dst_buffer, size_t dst_size,
+                           uint8_t* src_buffer, size_t src_size, int rfc1950);
+
+#pragma mark - CHECKSUMS
+
+// Return Crc32 of DATA[LEN]. Naive implementation.
+uint32_t simple_crc32(uint8_t* src_buffer, const size_t src_size);
+
+// Return Adler32 of DATA[LEN]. Naive implementation.
+uint32_t simple_adler32(const unsigned char* src, const size_t src_size);
--- a/AddOn/ZTests/ztest_verify.c	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZTests/ztest_verify.c	1695412879.000000000
@@ -0,0 +1,303 @@
+// ZLIB compression/decompression tests
+// CM 10/19/2022
+#include "ztest_common.h"
+
+#include <fcntl.h>
+#include <sys/stat.h>
+#include <dispatch/queue.h>
+#include <compression_private.h>
+
+#pragma mark - COMMAND LINE OPTIONS
+
+// Defaults for command line options
+static int use_fixed   = 0;   // Default to dymanic Huffman
+static int use_rfc1950 = 0;   // Use RFC 1950 (defaults to RFC 1951 == raw deflate w/o checksums)
+static int use_torture = 0;   // Torture stream API, checksums, truncated decodes w/ inflate/infback API
+static int n_its_enc   = 10;  // Number of encoder iterations
+static int n_its_dec   = 100; // Number of decoder iterations
+
+#pragma mark - Verify threaded decodes
+
+// Decodes buffer in multiple threads.
+void zlib_worker(const uint8_t* enc_buffer, const size_t enc_size,
+                 const uint8_t* src_buffer, const size_t src_size,
+                 const int use_rfc1950,
+                 const int use_fixed,
+                 const size_t idx)
+{
+  // Allocate data
+  void* data = calloc(1, src_size);
+  if (data == NULL) PFAIL("malloc");
+
+  // Decode and verify
+  const size_t decoded = zlib_decode_buffer(data, src_size, (uint8_t*)enc_buffer, enc_size, use_rfc1950);
+  if ((decoded != src_size) || (memcmp(data, src_buffer, src_size))) PFAIL("zlib_worker %zu", idx);
+
+  // Free data
+  free(data);
+}
+
+// Decodes in multiple threads. Triggers decoder bug found here: rdar://102994670
+void torture_threads(uint8_t* dst_buffer, size_t dst_size,
+                     uint8_t* src_buffer, size_t src_size, int use_rfc1950, int use_fixed)
+{
+  const size_t n_jobs = (1 << 30) / src_size; // Process 1GB
+  
+  // Encode using given options
+  const size_t enc_size = zlib_encode_buffer(dst_buffer, dst_size,
+                                             src_buffer, src_size, 5, use_rfc1950, use_fixed);
+  if (enc_size == 0) PFAIL("zlib_encode_buffer");
+  
+  // Decode with multiple threads
+  dispatch_apply(n_jobs, DISPATCH_APPLY_AUTO, ^(size_t idx)
+  {
+    zlib_worker(dst_buffer, enc_size,
+                src_buffer, src_size, use_rfc1950, use_fixed, idx);
+  });
+}
+
+#pragma mark - Verify CRC-32 / Adler32
+
+// Verify small buffer edge cases
+void torture_checksums(void* data, size_t size) // OK
+{
+  // Clamp buffer size?
+  if (size > 0x1000) size = 0x1000;
+  
+  // Torture Adler32 and Crc32...
+  for (size_t i = 0; i <= size; i++)
+  {
+    if (crc32_z(0, data, i) != simple_crc32(data, i)) PFAIL("CRC32");
+    if (adler32_z(1, data, i) != simple_adler32(data, i)) PFAIL("Adler-32");
+  }
+}
+
+// Make sure, that CRC32 produces expected results by ignoring bad high bits.
+static void verify_crc32_undefined_behavior(void)
+{
+  const Byte data[80] = {0};
+  
+  // CRC32 algorithm has a scalar and a vector part.
+  // - The vector part ignores the 32 upper bits by design.
+  // - The scalar part is processed before the vector part, to enforce alignment requirements.
+  // - We try different alignments, to trigger both parts of the algorithm.
+  for (int i = 0; i < 16; i++)
+  {
+    if (crc32_z(0x0000000000000000, data + i, 64) !=
+        crc32_z(0xffffffff00000000, data + i, 64)) PFAIL("CRC32 undefined behavior");
+  }
+}
+
+#pragma mark - BENCHMARK
+
+// Benchmark
+void process_file(const char* name) // OK
+{
+  uint8_t* src_buffer = NULL;
+  uint8_t* dst_buffer = NULL;
+  uint8_t* dec_buffer = NULL;
+  size_t src_size = 0;
+  size_t dst_size = 0;
+  size_t dec_size = 0;
+  size_t compressed_zlib = 0;
+  size_t compressed_comp = 0;
+  uint64_t cycles_crc   = 0;
+  uint64_t cycles_adler = 0;
+  uint64_t cycles_zlib_enc = 0;
+  uint64_t cycles_zlib_dec = 0;
+  uint64_t cycles_comp_enc = 0;
+  uint64_t cycles_comp_dec = 0;
+  uint32_t sum_crc   = 0;
+  uint32_t sum_adler = 0;
+  struct stat st;
+  int fd = -1;
+
+  //---------------------------------------------------------------------------- Setup benchmark
+  
+  // Open input and query size
+  fd = open(name, O_RDONLY, 0);
+  if (fd < 0) PFAIL("Can't open %s", name);
+  if (fstat(fd, &st) != 0) PFAIL("fstat");
+  src_size = st.st_size;
+  dst_size = (st.st_size * 9 >> 3) + 258; // Add extra space for already compressed data / small files
+
+  // Allocate buffers
+  src_buffer = malloc(src_size);
+  dst_buffer = malloc(dst_size);
+  dec_buffer = malloc(dst_size); // Don't hint decoder uncompressed size
+  if ((src_buffer == NULL) ||
+      (dst_buffer == NULL) ||
+      (dec_buffer == NULL)) PFAIL("malloc");
+
+  // Read data in GB chunks
+  for (size_t k, pos = 0; pos < src_size; pos += k)
+  {
+    k = 1 << 30;
+    if (pos + k > src_size) k = src_size - pos;
+    if (k != (size_t)read(fd, src_buffer + pos, k)) PFAIL("read");
+  }
+
+  //---------------------------------------------------------------------------- CHECKSUMS
+  // Benchmark Crc32
+  for (int i = 0; i <= n_its_dec; i++)
+  {
+    if (i <= 1) cycles_crc = kpc_get_cycles(); // Skip 0 == heat up
+    sum_crc = (uint32_t)crc32_z(0, src_buffer, src_size);
+  }
+  cycles_crc = kpc_get_cycles() - cycles_crc;
+  if (sum_crc != simple_crc32(src_buffer, src_size)) PFAIL("CRC32");
+
+  // Benchmark Adler32
+  for (int i = 0; i <= n_its_dec; i++)
+  {
+    if (i <= 1) cycles_adler = kpc_get_cycles(); // Skip 0 == heat up
+    sum_adler = (uint32_t)adler32_z(1, src_buffer, src_size);
+  }
+  cycles_adler = kpc_get_cycles() - cycles_adler;
+  if (sum_adler != simple_adler32(src_buffer, src_size)) PFAIL("Adler-32");
+  
+  //---------------------------------------------------------------------------- ENCODER COMPRESSION
+  for (int i = 0; i <= n_its_enc; i++)
+  {
+    if (i <= 1) cycles_comp_enc = kpc_get_cycles();
+    compressed_comp = compression_encode_buffer(dst_buffer, dst_size, src_buffer, src_size, NULL, use_rfc1950 ? COMPRESSION_ZLIB5_RFC1950 : COMPRESSION_ZLIB5);
+    if (compressed_comp == 0) PFAIL("compression_encode_buffer");
+  }
+  cycles_comp_enc = kpc_get_cycles() - cycles_comp_enc;
+  
+  //---------------------------------------------------------------------------- ENCODER ZLIB
+  for (int i = 0; i <= n_its_enc; i++)
+  {
+    if (i <= 1) cycles_zlib_enc = kpc_get_cycles();
+    compressed_zlib = zlib_encode_buffer(dst_buffer, dst_size, src_buffer, src_size, 5, use_rfc1950, use_fixed);
+    if (compressed_zlib == 0) PFAIL("zlib_encode_buffer");
+  }
+  cycles_zlib_enc = kpc_get_cycles() - cycles_zlib_enc;
+
+  //---------------------------------------------------------------------------- DECODER COMPRESSION
+  bzero(dec_buffer, dst_size);
+  for (int i = 0; i <= n_its_dec; i++)
+  {
+    if (i <= 1) cycles_comp_dec = kpc_get_cycles();
+    dec_size = compression_decode_buffer(dec_buffer, dst_size, dst_buffer, compressed_zlib, NULL, use_rfc1950 ? COMPRESSION_ZLIB_RFC1950 : COMPRESSION_ZLIB);
+    if (dec_size != src_size) PFAIL("compression_decode_buffer");
+  }
+  cycles_comp_dec = kpc_get_cycles() - cycles_comp_dec;
+  if (memcmp(src_buffer, dec_buffer, src_size)) PFAIL("data mismatch");
+  
+  //---------------------------------------------------------------------------- DECODER ZLIB
+  bzero(dec_buffer, dst_size);
+  for (int i = 0; i <= n_its_dec; i++)
+  {
+    if (i <= 1) cycles_zlib_dec = kpc_get_cycles();
+    dec_size = zlib_decode_buffer(dec_buffer, dst_size, dst_buffer, compressed_zlib, use_rfc1950);
+    if (dec_size != src_size) PFAIL("zlib_decode_buffer");
+  }
+  cycles_zlib_dec = kpc_get_cycles() - cycles_zlib_dec;
+  if (memcmp(src_buffer, dec_buffer, src_size)) PFAIL("data mismatch");
+
+  //---------------------------------------------------------------------------- TORTURE
+  if (use_torture)
+  {
+    // Torture checksums
+    torture_checksums(src_buffer, src_size);
+    
+    // Torture stream API
+    for (int i = 0; i <= n_its_dec; i++)
+    {
+      bzero(dec_buffer, src_size);
+      dec_size = zlib_decode_torture(dec_buffer, dst_size, dst_buffer, compressed_zlib, use_rfc1950);
+      if ((dec_size != src_size) || (memcmp(dec_buffer, src_buffer, src_size))) PFAIL("zlib_decode_torture");
+    }
+
+    // Truncated decodes
+    for (size_t size = 1;; size += (size + 31) >> 5)
+    {
+      // Clamp size?
+      if (size > src_size) size = src_size;
+      
+      // Torture inflate API
+      bzero(dec_buffer, size);
+      dec_size = zlib_decode_buffer(dec_buffer, size, dst_buffer, compressed_zlib, use_rfc1950);
+      if ((size != dec_size) || (memcmp(dec_buffer, src_buffer, size))) PFAIL("buffer torture, size=%zu", size);
+
+      // Torture infback API?
+      if (!use_rfc1950)
+      {
+        bzero(dec_buffer, size);
+        dec_size = zlib_decode_infback(dec_buffer, size, dst_buffer, compressed_zlib);
+        if ((size != dec_size) || (memcmp(dec_buffer, src_buffer, size))) PFAIL("infback torture, size=%zu", size);
+      }
+
+      // Done?
+      if (size == src_size) break;
+    }
+    
+    // Torture multi threaded decoding with and w/o fixed Huffman profile
+    // rdar://104176699
+    torture_threads(dst_buffer, dst_size, src_buffer, src_size, use_rfc1950, 0);
+    torture_threads(dst_buffer, dst_size, src_buffer, src_size, use_rfc1950, 1);
+  }
+  
+  //----------------------------------------------------------------------------
+  // Stats and cleanup
+  n_its_dec += (n_its_dec == 0); // Heat up == benchmark
+  n_its_enc += (n_its_enc == 0); // Heat up == benchmark
+  PLOG("%9zu B -> %9zu/%9zu B |"
+       " enc %6.2f/%6.2f c/B |"
+       " dec %5.2f/%5.2f c/B |"
+       " CRC32/Adler-32 %0.2f/%0.2f c/B %s| %s",
+       src_size, compressed_zlib, compressed_comp,
+       (float)cycles_zlib_enc / (n_its_enc * src_size),
+       (float)cycles_comp_enc / (n_its_enc * src_size),
+       (float)cycles_zlib_dec / (n_its_dec * src_size),
+       (float)cycles_comp_dec / (n_its_dec * src_size),
+       (float)cycles_crc      / (n_its_dec * src_size),
+       (float)cycles_adler    / (n_its_dec * src_size),
+       use_torture ? "| torture OK " : "",
+       name);
+  
+  // Free buffers and close file
+  free(src_buffer);
+  free(dst_buffer);
+  free(dec_buffer);
+  close(fd);
+}
+
+int show_help(const char* name) // OK
+{
+  PLOG("SYNTAX\n"
+       "\t%s [options] <file1> ... <fileN>\n"
+       "OPTIONS\n"
+       "\t-f\tUse fixed Huffman\n"
+       "\t-r\tUse RFC1950 (instead of RFC1951) and disable infback API\n"
+       "\t-t\tTorture truncated decodes (both APIs), streaming, CRC-32, Adler32 and threaded decoding\n"
+       "\t-e N\tTime N encoder iterations (default=%u)\n"
+       "\t-d N\tTime N decoder iterations (default=%u)\n"
+       , name, n_its_enc, n_its_dec);
+  exit(1);
+}
+
+int main(int argc, const char** argv)
+{
+  // Show syntax?
+  if (argc == 1) show_help(argv[0]);
+  
+  // Check CRC32
+  verify_crc32_undefined_behavior();
+  
+  // Iterate over files...
+  kpc_cycles_init();
+  for (int i = 1; i < argc; i++)
+  {
+    if (strcmp(argv[i],"-h") == 0) show_help(argv[0]);
+    if (strcmp(argv[i],"-f") == 0) { use_fixed   = 1; continue; }
+    if (strcmp(argv[i],"-r") == 0) { use_rfc1950 = 1; continue; }
+    if (strcmp(argv[i],"-t") == 0) { use_torture = 1; continue; }
+    if ((strcmp(argv[i],"-e") == 0) && (i + 1 < argc)) { n_its_enc = atoi(argv[++i]); continue; }
+    if ((strcmp(argv[i],"-d") == 0) && (i + 1 < argc)) { n_its_dec = atoi(argv[++i]); continue; }
+    process_file(argv[i]);
+  }
+  
+  return 0;
+}
--- a/AddOn/ZTests/ztest_zlib.c	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/ZTests/ztest_zlib.c	1695412879.000000000
@@ -0,0 +1,195 @@
+//
+//  t_zlib.c
+//  t_zlib
+//
+//  Created by Tal Uliel on 2/7/18.
+//
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <zlib.h>
+
+#define ZLIB_PADDING_SIZE (4 * 65536 + 8192) // Empirically determined alloc size for our deflateInit2 call + z_stream
+typedef struct {
+  
+  uint8_t * buf;          // pre-allocated buffer
+  size_t size;            // available bytes in BUF
+  
+} zlib_alloc_state;
+
+
+// Alloc item*size bytes. Return 0 on failure.
+// OPAQUE shall point to a zlib_alloc_state, and will be updated.
+static void * zlib_malloc(void * opaque,uInt items,uInt itemSize)
+{
+  zlib_alloc_state * s = (zlib_alloc_state *)opaque;
+  size_t size = (size_t)items*(size_t)itemSize;
+  
+  if ((size > 0) && (itemSize > 0) &&
+      ((size < items) || (size < itemSize)) )
+    return 0; //overflow
+  
+  // Fail if not enough space remaining in s->buf
+  if (size > s->size) return 0;
+  
+  // Return BUF, and consume SIZE bytes
+  void * ptr = s->buf;
+  s->buf += size;
+  s->size -= size;
+  return ptr;
+}
+
+// Free PTR.
+// OPAQUE shall point to a zlib_alloc_state, and will be updated.
+static void zlib_free(void * opaque,void * ptr)
+{
+  // We don't free anything
+}
+
+static int read_parameters(int argc, const char * argv[], FILE ** f)
+{
+  if (argc == 1)
+  {
+    fprintf(stderr, "t_zlib: error, input file wasn't specified\n");
+    return -1; // fail
+  }
+  
+  for (int i = 1; i < argc;)
+  {
+    if ((strcmp(argv[i],"-h")==0) || (strcmp(argv[i],"-help")==0))
+    {
+      return 1; // need to print help
+    } else
+    {
+      *f = fopen(argv[i], "rb");
+      if (*f == 0)
+      {
+        fprintf(stderr,"t_zlib: failed to open %s\n", argv[i]);
+        return -1; // fail
+      }
+      if ((i+1) != argc)
+      {
+        fprintf(stderr, "t_zlib: ignoring rest of the paramters after file path %s\n", argv[i]);
+        return 0; // success
+      }
+      i++;
+    }
+  }
+  
+  return 0;
+}
+
+static void print_help()
+{
+  fprintf(stderr, "t_zlib help\n");
+  fprintf(stderr, "t_zlib <path_to_file>\n");
+  fprintf(stderr, "t_zlib will load content from a file and verify encode/decode\n");
+  fprintf(stderr, "-h/-help - print help message");
+}
+
+/*!
+ @abstract simple tester for zlib
+ @discussion simple tester to load content from a file and verify encode/decode
+ */
+int main(int argc, const char * argv[])
+{
+  FILE * f;
+  int res = read_parameters(argc, argv, &f);
+  
+  if (res)
+  {
+    print_help();
+    
+    // return 0 if user requested to print help menu (-h/-help)
+    return (res < 0);
+  }
+  
+  fseek(f, 0, SEEK_END);
+  size_t number_of_bytes = ftell(f);
+  Bytef * input_buf   = malloc(number_of_bytes); // buffer that will hold the input byts from the file
+  if (input_buf == 0) { fprintf(stderr, "t_zlib: alloc failed\n"); return -1;}
+
+  // read input from file
+  fseek(f, 0, SEEK_SET);
+  fread(input_buf, 1, number_of_bytes, f);
+
+  // close the file
+  fclose(f);
+
+  // assuming 2x the input size + ZLIB_PADDING_SIZE would be enough
+  Bytef * encoded_buf = malloc(2*number_of_bytes + ZLIB_PADDING_SIZE); // buffer to hold the encode result
+  if (encoded_buf == 0) { fprintf(stderr, "t_zlib: alloc failed\n"); return -1;}
+  
+  // set stream structre for encode
+  z_stream desc;
+  bzero(&desc, sizeof(desc));
+  
+  uint8_t scratch[ZLIB_PADDING_SIZE];
+  zlib_alloc_state opaque;
+  opaque.buf = (uint8_t*)scratch;
+  opaque.size = ZLIB_PADDING_SIZE;
+  
+  desc.next_in = input_buf;
+  desc.avail_in = (uint32_t)number_of_bytes;
+  desc.next_out = encoded_buf;
+  desc.avail_out = (uint32_t)(2*number_of_bytes + ZLIB_PADDING_SIZE);
+  desc.zalloc = zlib_malloc;
+  desc.zfree = zlib_free;
+  desc.opaque = &opaque;
+  
+  // encode file
+  res = deflateInit2(&desc, 7, Z_DEFLATED,(-15),1,Z_DEFAULT_STRATEGY);
+  
+  res = deflate(&desc, Z_FINISH);
+  if (res != Z_STREAM_END) { fprintf(stderr, "zlib: encode failed\n"); return -1; }
+
+  size_t encoded_size = desc.total_out;
+  deflateEnd(&desc);
+  
+  // realloc encoede buf to limit buffer size
+  encoded_buf = realloc(encoded_buf, encoded_size);
+  
+  Bytef * decoded_buf = malloc(number_of_bytes); // bufert hat will hold the decode result
+  if (decoded_buf == 0) { fprintf(stderr, "t_zlib: alloc failed\n"); return -1;}
+
+  // set stream structre for encode
+  bzero(&desc, sizeof(desc));
+  opaque.buf = (uint8_t*)scratch;
+  opaque.size = ZLIB_PADDING_SIZE;
+
+  desc.next_in = encoded_buf;
+  desc.avail_in = (uint32_t)encoded_size;
+  desc.next_out = decoded_buf;
+  desc.avail_out = (uint32_t)number_of_bytes;
+  desc.zalloc = zlib_malloc;
+  desc.zfree = zlib_free;
+  desc.opaque = &opaque;
+  desc.adler = (uint32_t)adler32(0, input_buf, (uint32_t)number_of_bytes);
+
+  // decode buffer
+  inflateInit2(&desc, (-15));
+
+  res = inflate(&desc, Z_FINISH);
+  if (res != Z_STREAM_END) { fprintf(stderr, "zlib: decode failed\n"); return -1; }
+  
+  inflateEnd(&desc);
+
+  free(encoded_buf);
+
+  // compare buffers
+  for (size_t i = 0; i < number_of_bytes; i++)
+  {
+    if (input_buf[i] != decoded_buf[i])
+    {
+      fprintf(stderr, "t_zlib: valdiation fail, buffers differ in byte %zu\n", i);
+      return -1;
+    }
+  }
+  
+  free(input_buf);
+  free(decoded_buf);
+  
+  fprintf(stderr, "PASSED\n");
+  return 0;
+}
--- a/AddOn/zopt_defs.h	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/zopt_defs.h	1695412879.000000000
@@ -0,0 +1,60 @@
+#pragma once
+
+// Dirty way to disable optimizations
+//#undef VEC_OPTIMIZE
+//#undef INFFAST_OPT
+
+#pragma mark - CRC32 and ADLER32
+
+#ifdef VEC_OPTIMIZE
+
+extern uLong adler32_vec(unsigned int adler, unsigned int sum2, const Bytef* buf, int len);
+extern uint32_t crc32_little_aligned_vector(uint32_t crc, const unsigned char *buf, uint32_t len);
+
+#endif
+
+#pragma mark - INFFAST
+
+#ifdef INFFAST_OPT
+
+typedef __attribute__((__ext_vector_type__(16))) uint8_t vector_uchar16;
+typedef __attribute__((__ext_vector_type__(16),__aligned__(1))) uint8_t packed_uchar16;
+typedef __attribute__((__ext_vector_type__(8),__aligned__(2))) unsigned short packed_ushort8;
+typedef __attribute__((__aligned__(1))) uint64_t packed_uint64_t;
+
+// Build Huffman tables. Return 0 on success, 1 on error.
+int inffast_tables(z_streamp strm);
+
+#define ASMINF // Disable inffast
+#undef  INFLATE_MIN_INPUT
+#define INFLATE_MIN_INPUT ((15+15+(15+5)+(15+13)+64+64)/8) // Be safe, use generous upper bound
+#undef  INFLATE_MIN_OUTPUT
+#define INFLATE_MIN_OUTPUT (2+258+63) // 2 literals, match + overshoot
+
+/*
+ zlib defaults to 9 lenbits and 6 distbits.  Thus, zlib reserves ENOUGH (=1444)
+ codes in its state.  We try to use larger Huffman lookup tables.  Usually,
+ 1444 codes are enough for 10 lenbits and 7 distbits.  If tree construction
+ fails, we fallback to the default values.
+ */
+#define INFLATE_LEN_BITS_OPT 10 // See ENOUGH*, max 1334 entries needed
+#define INFLATE_DIST_BITS_OPT 7 // See ENOUGH*, max 400 entries needed
+
+/*
+ INFFAST_OPT decodes the next Huffman symbol while processing the current one.
+ To step over the current symbol, we need to consume the extra bits of a
+ length/distance symbol in one step.  Therefore, we modify the Huffman codes.
+ We need to transform the codes back for the classic "wrapper" code to work.
+ */
+#define INFLATE_ADD_EXTRA_BITS(here)                  \
+    {                                                 \
+    const int8_t mask = -((here.op >> 4) & 1) & 15;   \
+    here.bits += here.op & mask; /* add extra bits */ \
+    }
+#define INFLATE_SUB_EXTRA_BITS(here)                  \
+    {                                                 \
+    const int8_t mask = -((here.op >> 4) & 1) & 15;   \
+    here.bits -= here.op & mask; /* sub extra bits */ \
+    }
+
+#endif
--- a/AddOn/zopt_inffast.c	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/zopt_inffast.c	1695412879.000000000
@@ -0,0 +1,353 @@
+#include "../zutil.h"
+
+#if defined(INFFAST_OPT)
+
+/*
+ Entry assumptions:
+    state->mode == LEN
+    strm->avail_in >= INFLATE_MIN_INPUT
+    strm->avail_out >= INFLATE_MIN_OUTPUT
+    start >= strm->avail_out
+    state->bits <= 8
+ 
+ On return, state->mode is one of:
+    LEN -- ran out of enough output space or enough available input
+    TYPE -- reached end of block code, inflate() to interpret next block
+    BAD -- error in block data
+ 
+ Other
+    See inffast.c for more information.
+    Critical path comments:
+      - assume LENBITS=11 and DISTBITS=8
+      - e = extended lookup, x = extra bits
+ */
+
+#include "../inffast.h"
+#include "../inftrees.h"
+#include "../inflate.h"
+#include "zopt_inffast.h"
+
+#define INFLATE_REFILL(new_bits)                                 \
+  {                                                              \
+    new_bits = (*(packed_uint64_t*)src_buffer << n_bits) | bits; \
+    src_buffer += (63 - n_bits) >> 3;                            \
+    n_bits |= 56;                                                \
+  }
+
+#define INFLATE_DECODE(sym, table, index) \
+  {                                       \
+    sym = table[index];                   \
+    /* no reordering by compiler */       \
+    asm volatile ("" : "+r"(sym));        \
+  }
+
+#define INFLATE_CONSUME(sym)      \
+  {                               \
+    bits_symbol = (uint32_t)bits; \
+    bits >>= sym.bits;            \
+    n_bits -= sym.bits;           \
+  }
+
+void ZLIB_INTERNAL inflate_fast(z_streamp strm, unsigned start) /* inflate()'s starting value for strm->avail_out */
+{
+  struct inflate_state* state;      // Internal state
+  
+  const unsigned char* src_buffer;  // Payload
+  const unsigned char* src_safe;    // Payload, safe for fast loop
+  const unsigned char* src_end;     // End of payload
+  
+  unsigned char* dst_buffer;        // Output
+  unsigned char* dst_begin;         // Start of output
+  unsigned char* dst_safe;          // Output, safe for fast loop
+  unsigned char* dst_end;           // End of output
+  
+  const code* bits_to_lenlit;       // Huffman LUTs for lengths/literals
+  const code* bits_to_distance;     // Huffman LUTs for distances
+  uint32_t lenlit_mask;             // Mask for lengths/literals root table
+  uint32_t distance_mask;           // Mask for distances root table
+  
+  uint64_t bits;                    // Bits available for next decode
+  uint32_t bits_symbol;             // Consumed bits of last symbol *_SYM
+  uint32_t n_bits;                  // # of bits available in BITS
+  
+  const unsigned char* window;      // Sliding window, if wsize > 0
+  uint32_t win_size;                // Size of sliding window
+  uint32_t win_top;                 // Sliding window: next write
+  
+  // Copy state
+  state = (struct inflate_state*)strm->state;
+  
+  // Setup buffers
+  src_buffer = strm->next_in;
+  dst_buffer = strm->next_out;
+  src_end    = src_buffer + strm->avail_in;
+  dst_end    = dst_buffer + strm->avail_out;
+  src_safe   = src_end - INFLATE_MIN_INPUT;
+  dst_safe   = dst_end - INFLATE_MIN_OUTPUT;
+  dst_begin  = dst_end - start;
+  
+  // Setup window
+  window   = state->window;
+  win_size = state->wsize;
+  win_top  = (state->whave >= win_size) && (state->wnext == 0) ? win_size : state->wnext;
+  
+  // Setup Huffman tables
+  bits_to_lenlit   = state->lencode;
+  bits_to_distance = state->distcode;
+  lenlit_mask   = ~(-1 << state->lenbits);
+  distance_mask = ~(-1 << state->distbits);
+
+  // Refill and enforce some assumptions
+  n_bits = state->bits & 63;
+  bits   = state->hold & ~(UINT64_C(-1) << n_bits);
+  INFLATE_REFILL(bits);
+  
+  //---------------------------------------------------------------------------- Inner loop
+  // Decode until unsafe, end of block or distance error
+  for (;;)
+  {
+    code l_sym;                 // Current/pending length/literal Huffman symbol
+    code d_sym;                 // Current/pending distance       Huffman symbol
+    uint32_t length;            // Match length
+    uint32_t distance;          // Match distance
+
+    // Decode length/literal symbol and refill bits
+    INFLATE_DECODE(l_sym, bits_to_lenlit, bits & lenlit_mask);
+    INFLATE_REFILL(bits); // We have 56+ bits
+    INFLATE_CONSUME(l_sym);
+
+  lenlit_decode:
+    //-------------------------------------------------------------------------- LEN/LIT: Root literal?
+    if (likely(l_sym.op == 0))
+    {
+      *dst_buffer++ = (unsigned char)l_sym.val;
+      
+      // Decode length/literal symbol
+      INFLATE_DECODE(l_sym, bits_to_lenlit, bits & lenlit_mask);
+      INFLATE_CONSUME(l_sym);
+      
+      //------------------------------------------------------------------------ LEN/LIT: Root literal?
+      if (likely(l_sym.op == 0))
+      {
+        *dst_buffer++ = (unsigned char)l_sym.val;
+        
+        // Decode length/literal symbol
+        INFLATE_DECODE(l_sym, bits_to_lenlit, bits & lenlit_mask);
+        INFLATE_CONSUME(l_sym);
+
+      lit_try_extended:
+        //---------------------------------------------------------------------- LEN/LIT: Another literal?
+        if (likely(l_sym.op == 0))
+        {
+          // Critical path: (11) + (11) + (11+4e) = 37 bits
+          *dst_buffer++ = (unsigned char)l_sym.val;
+          
+          // Safe to continue?
+          if (likely((src_buffer <= src_safe) && (dst_buffer <= dst_safe))) continue;
+          
+          // Unsafe, stop.
+          break;
+        }
+      }
+    }
+    const uint8_t lenlit_op = (unsigned)l_sym.op;
+    //-------------------------------------------------------------------------- LEN/LIT: Length decodable?
+    if (likely(lenlit_op & 16))
+    {
+      // Decode distance symbol (root)
+      // Critical path: (11) + (11) + (11+4e+5x) = 42 bits
+      // Since we started with 56+ bits, we are still good.
+      INFLATE_DECODE(d_sym, bits_to_distance, bits & distance_mask);
+
+      // The complete distance needs up to 28 (8+7e+13x) bits.
+      if (unlikely(n_bits < 15 + 13))
+      {
+        INFLATE_REFILL(bits); // (**)
+      }
+
+      // Decode length using base and extra bits
+      length = ((bits_symbol << (32 - l_sym.bits)) >> 1 >> (31 - (lenlit_op & 15))) + l_sym.val;
+
+      // Consume distance
+      INFLATE_CONSUME(d_sym);
+
+    distance_decode:;
+      const uint8_t distance_op = (unsigned)(d_sym.op);
+      //------------------------------------------------------------------------ DISTANCE: Distance decodable?
+      if (likely(distance_op & 16))
+      {
+        // Refill for length/literal decode?
+        if (unlikely(n_bits < INFLATE_LEN_BITS_OPT))
+        {
+          INFLATE_REFILL(bits); // It's unlikely, that we don't have enough bits for the lookup.
+        }
+
+        // Decode distance using base and extra bits
+        distance = ((bits_symbol << (32 - d_sym.bits)) >> 1 >> (31 - (distance_op & 15))) + d_sym.val;
+        
+        // Decode length/literal symbol (root) and refill bits.
+        bits_symbol = (uint32_t)bits;
+        INFLATE_REFILL(bits); // Buffer bits for next iteration / revert
+        INFLATE_DECODE(l_sym, bits_to_lenlit, bits_symbol & lenlit_mask);
+        INFLATE_CONSUME(l_sym);
+        
+        //---------------------------------------------------------------------- Match copy
+        // Copy from window?
+        const uint32_t dst_available = (uint32_t)(dst_buffer - dst_begin);
+        if (distance > dst_available)
+        {
+          const uint32_t win_distance = distance - dst_available;
+          uint32_t match_pos = win_top - win_distance;
+
+          // Match starts in [0, win_top)?
+          if ((int32_t)match_pos >= 0)
+          {
+            // Some output needed?
+            if (unlikely(match_pos + length > win_top)) goto match_copy_edge_cases;
+          }else
+          {
+            // Match starts in [win_top, win_size)
+            match_pos += win_size;
+
+            // Out of window?
+            if (unlikely((int32_t)match_pos < (int32_t)win_top))
+            {
+              strm->msg = (z_const char*)"invalid distance too far back";
+              state->mode = BAD;
+              break;
+            }
+          }
+          
+          // Unsafe or wrap around?
+          if (unlikely(match_pos + length + 63 > win_size)) goto match_copy_edge_cases;
+
+          // Use simple copy
+          dst_buffer = inflate_copy_fast(dst_buffer, window + match_pos, length);
+          goto match_copy_done;
+          
+          //----------------------------------------------------------------------
+          // This loop covers edge cases, where we start within WINDOW
+          //    1) Wrap around in WINDOW
+          //    2) Wrap around to DST_BUFFER
+          //    3) Unsafe copies
+          //----------------------------------------------------------------------
+        match_copy_edge_cases:
+          for (;;)
+          {
+            *dst_buffer++ = window[match_pos++];
+            if (--length == 0) goto match_copy_done;  // Done
+            if (match_pos == win_top) break;          // Wrap around to DST_BUFFER
+            if (match_pos == win_size) match_pos = 0; // Wrap around in WINDOW
+          }
+        }
+
+        // Copy from DST_BUFFER (handle overlapping copies)
+        dst_buffer = (length <= distance ?
+                      inflate_copy_fast(dst_buffer, dst_buffer - distance, length) :
+                      (distance < 16 ?
+                       inflate_copy_with_overlap_small(dst_buffer, distance, length) :
+                       inflate_copy_with_overlap_large(dst_buffer, distance, length)));
+        
+      match_copy_done:
+        // Safe to continue?
+        if (likely((src_buffer <= src_safe) && (dst_buffer <= dst_safe))) goto lenlit_decode;
+
+        // Unsafe, revert pending decode and stop.
+        n_bits += l_sym.bits;
+        bits = bits_symbol;
+        break;
+      }
+      //------------------------------------------------------------------------ DISTANCE: Need second level table?
+      else if (likely((distance_op & 64) == 0))
+      {
+        // Decode distance symbol
+        INFLATE_DECODE(d_sym, bits_to_distance, ((uint32_t)bits & ~(-1 << distance_op)) + d_sym.val);
+        INFLATE_CONSUME(d_sym); // Critical path: we have enough bits, see (**)
+        goto distance_decode;
+      }
+      // Bitstream error (***)
+    }
+    //-------------------------------------------------------------------------- LEN/LIT: Need second level table?
+    else if (likely((lenlit_op & 64) == 0))
+    {
+      // Decode length/literal symbol
+      INFLATE_DECODE(l_sym, bits_to_lenlit, ((uint32_t)bits & ~(-1 << lenlit_op)) + l_sym.val);
+      INFLATE_CONSUME(l_sym);
+      goto lit_try_extended;
+    }
+    //-------------------------------------------------------------------------- LEN/LIT: End of block?
+    else if (lenlit_op & 32)
+    {
+      state->mode = TYPE;
+      break;
+    }
+    //-------------------------------------------------------------------------- LEN/LIT/DISTANCE: Bitstream error (***)
+    {
+      strm->msg = (char*)"invalid literal/length/distance code";
+      state->mode = BAD;
+      break;
+    }
+  };
+  
+  // Return unused bytes/bits
+  src_buffer -= n_bits >> 3; // Bytes
+  n_bits &= 7;
+  state->bits = n_bits;
+  state->hold = (uint32_t)bits & ~(-1 << n_bits);
+
+  // Update state
+  strm->next_in   = (z_const unsigned char*)src_buffer;
+  strm->next_out  = dst_buffer;
+  strm->avail_in  = (unsigned int)(src_end - src_buffer);
+  strm->avail_out = (unsigned int)(dst_end - dst_buffer);
+}
+
+// Build Huffman tables. Return 0 on success, 1 on error.
+int inffast_tables(z_streamp strm) // OK
+{
+  struct inflate_state* state = (struct inflate_state*)strm->state;
+  int ret = 0;
+  
+  // Try to build larger tables
+  state->lenbits = INFLATE_LEN_BITS_OPT;
+  state->distbits = INFLATE_DIST_BITS_OPT;
+
+  // Build tables...
+  for (;;)
+  {
+    // Build length codes
+    state->next = state->codes;
+    state->lencode = state->next;
+    ret = inflate_table(LENS, state->lens, state->nlen,
+                        &(state->next), &(state->lenbits), state->work,
+                        (unsigned)(state->codes + ENOUGH - state->next));
+    if (ret)
+    {
+      strm->msg = (char *)"invalid literal/lengths set";
+      state->mode = BAD;
+      return 1;
+    }
+    
+    // Build distance codes
+    state->distcode = state->next;
+    ret = inflate_table(DISTS, state->lens + state->nlen, state->ndist,
+                        &(state->next), &(state->distbits), state->work,
+                        (unsigned)(state->codes + ENOUGH - state->next));
+    if (ret == 0) return 0; // Done
+
+    // Fallback to default table sizes?
+    if ((state->lenbits > INFLATE_LEN_BITS) || (state->distbits > INFLATE_DIST_BITS))
+    {
+      state->lenbits = INFLATE_LEN_BITS;
+      state->distbits = INFLATE_DIST_BITS;
+    }else
+    {
+      // Bad distance set
+      strm->msg = (char *)"invalid distances set";
+      state->mode = BAD;
+      return 1;
+    }
+  }
+}
+
+#endif
--- a/AddOn/zopt_inffast.h	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/zopt_inffast.h	1695412879.000000000
@@ -0,0 +1,119 @@
+#pragma once
+
+#include "zopt_defs.h"
+
+#if defined(INFFAST_OPT)
+
+#if defined(__SSE2__)
+#include <immintrin.h>
+#endif
+
+#if defined __arm64__
+#include <arm_neon.h>
+#endif
+
+#define INFLATE_INLINE static inline __attribute__((__always_inline__)) __attribute__((__overloadable__))
+
+#ifndef __has_builtin
+# define __has_builtin(x) 0
+#endif
+
+#if __has_builtin(__builtin_expect)
+# define likely(x)    __builtin_expect((x),1)
+# define unlikely(x)  __builtin_expect((x),0)
+#else
+# define likely(x)    (x)
+# define unlikely(x)  (x)
+#endif
+
+#pragma mark - MATCH COPY
+
+// Shuffle vector X using permutation PERM.
+INFLATE_INLINE vector_uchar16 inflate_shuffle(vector_uchar16 x, vector_uchar16 perm) // OK
+{
+#if defined(__SSE2__)
+  return _mm_shuffle_epi8(x, perm);
+#elif defined(__arm64__)
+  return vqtbl1q_u8(x, perm);
+#endif
+}
+
+// Copy with overlap and distance < 16. Can overshoot by 47 bytes.
+INFLATE_INLINE uint8_t* inflate_copy_with_overlap_small(uint8_t* dst, const size_t distance, const size_t len) // OK
+{
+  Assert((distance) && (distance < 16), "bad overlapping distance");
+  uint8_t* dst_end = dst + len;
+#if defined(__SSE2__) || defined(__arm64__)
+  #define _ 0
+  const static vector_uchar16 repeat_perm[15] = {
+    {0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0, 0},
+    {0, 1, 0, 1, 0, 1, 0, 1, 0, 1,  0,  1,  0,  1,  0, 1},
+    {0, 1, 2, 0, 1, 2, 0, 1, 2, 0,  1,  2,  0,  1,  2, _},
+    {0, 1, 2, 3, 0, 1, 2, 3, 0, 1,  2,  3,  0,  1,  2, 3},
+    {0, 1, 2, 3, 4, 0, 1, 2, 3, 4,  0,  1,  2,  3,  4, _},
+    {0, 1, 2, 3, 4, 5, 0, 1, 2, 3,  4,  5,  _,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 0, 1, 2,  3,  4,  5,  6,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 0, 1,  2,  3,  4,  5,  6, 7},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, _,  _,  _,  _,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  _,  _,  _,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  _,  _,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,  _,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,  _,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,  _, _},
+    {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, _},
+  };
+  #undef _
+  const static uint8_t repeat_size[15] = {16, 16, 15, 16, 15, 12, 14, 16, 9, 10, 11, 12, 13, 14, 15};
+  const vector_uchar16 pattern = inflate_shuffle(*(packed_uchar16*)(dst - distance), repeat_perm[distance - 1]);
+  const size_t pattern_size = repeat_size[distance - 1];
+  const uint8_t* src = dst - distance;
+
+  *(packed_uchar16*)(dst                    ) = pattern;
+  *(packed_uchar16*)(dst + pattern_size     ) = *(packed_uchar16*)(src     ); // distance + pattern_size >= 16
+  *(packed_uchar16*)(dst + pattern_size + 16) = *(packed_uchar16*)(src + 16);
+  src -= pattern_size;
+  for (size_t i = pattern_size + 32; (i < len); i += 16)
+  {
+    *(packed_uchar16*)(dst + i) = *(packed_uchar16*)(src + i);
+  }
+#else
+  // No shuffle available, copy byte by byte.
+  do { *dst = *(dst - distance); } while (++dst < dst_end);
+#endif
+  return dst_end;
+}
+
+// Copy with overlap and distance >= 16. Can overshoot by 47 bytes.
+INFLATE_INLINE uint8_t* inflate_copy_with_overlap_large(uint8_t* dst, const size_t distance, uint32_t len) // OK
+{
+  Assert(distance >= 16, "bad overlapping distance");
+  const uint8_t* src = dst - distance;
+  uint8_t* dst_end = dst + len;
+  
+  *(packed_uchar16*)(dst     ) = *(packed_uchar16*)(src     );
+  *(packed_uchar16*)(dst + 16) = *(packed_uchar16*)(src + 16);
+  *(packed_uchar16*)(dst + 32) = *(packed_uchar16*)(src + 32);
+  for (size_t i = 48; (i < len); i += 16)
+  {
+    *(packed_uchar16*)(dst + i) = *(packed_uchar16*)(src + i);
+  }
+  return dst_end;
+}
+
+// Copy without overlap. Can overshoot 63 bytes.
+INFLATE_INLINE uint8_t* inflate_copy_fast(uint8_t* dst, const uint8_t* restrict src, const uint32_t len) // OK
+{
+  uint8_t* dst_end = dst + len;
+  
+  *(packed_uchar16*)(dst +  0) = *(packed_uchar16*)(src +  0);
+  *(packed_uchar16*)(dst + 16) = *(packed_uchar16*)(src + 16);
+  *(packed_uchar16*)(dst + 32) = *(packed_uchar16*)(src + 32);
+  *(packed_uchar16*)(dst + 48) = *(packed_uchar16*)(src + 48);
+  for (size_t i = 64; (i < len); i += 16)
+  {
+    *(packed_uchar16*)(dst + i) = *(packed_uchar16*)(src + i);
+  }
+  return dst_end;
+}
+
+#endif
--- a/AddOn/zopt_inffixed.h	1900-01-00 00:00:00.000000000 +0000
+++ b/AddOn/zopt_inffixed.h	1695412879.000000000
@@ -0,0 +1,94 @@
+    /* inffixed.h -- table for decoding fixed codes
+     * Generated automatically by makefixed().
+     */
+
+    /* WARNING: this file should *not* be used by applications.
+       It is part of the implementation of this library and is
+       subject to change. Applications should only use zlib.h.
+     */
+
+    static const code lenfix[512] = {
+        {7,96,0},{8,0,80},{8,0,16},{12,20,115},{9,18,31},{8,0,112},{8,0,48},
+        {9,0,192},{7,16,10},{8,0,96},{8,0,32},{9,0,160},{8,0,0},{8,0,128},
+        {8,0,64},{9,0,224},{7,16,6},{8,0,88},{8,0,24},{9,0,144},{10,19,59},
+        {8,0,120},{8,0,56},{9,0,208},{8,17,17},{8,0,104},{8,0,40},{9,0,176},
+        {8,0,8},{8,0,136},{8,0,72},{9,0,240},{7,16,4},{8,0,84},{8,0,20},
+        {13,21,227},{10,19,43},{8,0,116},{8,0,52},{9,0,200},{8,17,13},{8,0,100},
+        {8,0,36},{9,0,168},{8,0,4},{8,0,132},{8,0,68},{9,0,232},{7,16,8},
+        {8,0,92},{8,0,28},{9,0,152},{11,20,83},{8,0,124},{8,0,60},{9,0,216},
+        {9,18,23},{8,0,108},{8,0,44},{9,0,184},{8,0,12},{8,0,140},{8,0,76},
+        {9,0,248},{7,16,3},{8,0,82},{8,0,18},{13,21,163},{10,19,35},{8,0,114},
+        {8,0,50},{9,0,196},{8,17,11},{8,0,98},{8,0,34},{9,0,164},{8,0,2},
+        {8,0,130},{8,0,66},{9,0,228},{7,16,7},{8,0,90},{8,0,26},{9,0,148},
+        {11,20,67},{8,0,122},{8,0,58},{9,0,212},{9,18,19},{8,0,106},{8,0,42},
+        {9,0,180},{8,0,10},{8,0,138},{8,0,74},{9,0,244},{7,16,5},{8,0,86},
+        {8,0,22},{8,64,0},{10,19,51},{8,0,118},{8,0,54},{9,0,204},{8,17,15},
+        {8,0,102},{8,0,38},{9,0,172},{8,0,6},{8,0,134},{8,0,70},{9,0,236},
+        {7,16,9},{8,0,94},{8,0,30},{9,0,156},{11,20,99},{8,0,126},{8,0,62},
+        {9,0,220},{9,18,27},{8,0,110},{8,0,46},{9,0,188},{8,0,14},{8,0,142},
+        {8,0,78},{9,0,252},{7,96,0},{8,0,81},{8,0,17},{13,21,131},{9,18,31},
+        {8,0,113},{8,0,49},{9,0,194},{7,16,10},{8,0,97},{8,0,33},{9,0,162},
+        {8,0,1},{8,0,129},{8,0,65},{9,0,226},{7,16,6},{8,0,89},{8,0,25},
+        {9,0,146},{10,19,59},{8,0,121},{8,0,57},{9,0,210},{8,17,17},{8,0,105},
+        {8,0,41},{9,0,178},{8,0,9},{8,0,137},{8,0,73},{9,0,242},{7,16,4},
+        {8,0,85},{8,0,21},{8,16,258},{10,19,43},{8,0,117},{8,0,53},{9,0,202},
+        {8,17,13},{8,0,101},{8,0,37},{9,0,170},{8,0,5},{8,0,133},{8,0,69},
+        {9,0,234},{7,16,8},{8,0,93},{8,0,29},{9,0,154},{11,20,83},{8,0,125},
+        {8,0,61},{9,0,218},{9,18,23},{8,0,109},{8,0,45},{9,0,186},{8,0,13},
+        {8,0,141},{8,0,77},{9,0,250},{7,16,3},{8,0,83},{8,0,19},{13,21,195},
+        {10,19,35},{8,0,115},{8,0,51},{9,0,198},{8,17,11},{8,0,99},{8,0,35},
+        {9,0,166},{8,0,3},{8,0,131},{8,0,67},{9,0,230},{7,16,7},{8,0,91},
+        {8,0,27},{9,0,150},{11,20,67},{8,0,123},{8,0,59},{9,0,214},{9,18,19},
+        {8,0,107},{8,0,43},{9,0,182},{8,0,11},{8,0,139},{8,0,75},{9,0,246},
+        {7,16,5},{8,0,87},{8,0,23},{8,64,0},{10,19,51},{8,0,119},{8,0,55},
+        {9,0,206},{8,17,15},{8,0,103},{8,0,39},{9,0,174},{8,0,7},{8,0,135},
+        {8,0,71},{9,0,238},{7,16,9},{8,0,95},{8,0,31},{9,0,158},{11,20,99},
+        {8,0,127},{8,0,63},{9,0,222},{9,18,27},{8,0,111},{8,0,47},{9,0,190},
+        {8,0,15},{8,0,143},{8,0,79},{9,0,254},{7,96,0},{8,0,80},{8,0,16},
+        {12,20,115},{9,18,31},{8,0,112},{8,0,48},{9,0,193},{7,16,10},{8,0,96},
+        {8,0,32},{9,0,161},{8,0,0},{8,0,128},{8,0,64},{9,0,225},{7,16,6},
+        {8,0,88},{8,0,24},{9,0,145},{10,19,59},{8,0,120},{8,0,56},{9,0,209},
+        {8,17,17},{8,0,104},{8,0,40},{9,0,177},{8,0,8},{8,0,136},{8,0,72},
+        {9,0,241},{7,16,4},{8,0,84},{8,0,20},{13,21,227},{10,19,43},{8,0,116},
+        {8,0,52},{9,0,201},{8,17,13},{8,0,100},{8,0,36},{9,0,169},{8,0,4},
+        {8,0,132},{8,0,68},{9,0,233},{7,16,8},{8,0,92},{8,0,28},{9,0,153},
+        {11,20,83},{8,0,124},{8,0,60},{9,0,217},{9,18,23},{8,0,108},{8,0,44},
+        {9,0,185},{8,0,12},{8,0,140},{8,0,76},{9,0,249},{7,16,3},{8,0,82},
+        {8,0,18},{13,21,163},{10,19,35},{8,0,114},{8,0,50},{9,0,197},{8,17,11},
+        {8,0,98},{8,0,34},{9,0,165},{8,0,2},{8,0,130},{8,0,66},{9,0,229},
+        {7,16,7},{8,0,90},{8,0,26},{9,0,149},{11,20,67},{8,0,122},{8,0,58},
+        {9,0,213},{9,18,19},{8,0,106},{8,0,42},{9,0,181},{8,0,10},{8,0,138},
+        {8,0,74},{9,0,245},{7,16,5},{8,0,86},{8,0,22},{8,64,0},{10,19,51},
+        {8,0,118},{8,0,54},{9,0,205},{8,17,15},{8,0,102},{8,0,38},{9,0,173},
+        {8,0,6},{8,0,134},{8,0,70},{9,0,237},{7,16,9},{8,0,94},{8,0,30},
+        {9,0,157},{11,20,99},{8,0,126},{8,0,62},{9,0,221},{9,18,27},{8,0,110},
+        {8,0,46},{9,0,189},{8,0,14},{8,0,142},{8,0,78},{9,0,253},{7,96,0},
+        {8,0,81},{8,0,17},{13,21,131},{9,18,31},{8,0,113},{8,0,49},{9,0,195},
+        {7,16,10},{8,0,97},{8,0,33},{9,0,163},{8,0,1},{8,0,129},{8,0,65},
+        {9,0,227},{7,16,6},{8,0,89},{8,0,25},{9,0,147},{10,19,59},{8,0,121},
+        {8,0,57},{9,0,211},{8,17,17},{8,0,105},{8,0,41},{9,0,179},{8,0,9},
+        {8,0,137},{8,0,73},{9,0,243},{7,16,4},{8,0,85},{8,0,21},{8,16,258},
+        {10,19,43},{8,0,117},{8,0,53},{9,0,203},{8,17,13},{8,0,101},{8,0,37},
+        {9,0,171},{8,0,5},{8,0,133},{8,0,69},{9,0,235},{7,16,8},{8,0,93},
+        {8,0,29},{9,0,155},{11,20,83},{8,0,125},{8,0,61},{9,0,219},{9,18,23},
+        {8,0,109},{8,0,45},{9,0,187},{8,0,13},{8,0,141},{8,0,77},{9,0,251},
+        {7,16,3},{8,0,83},{8,0,19},{13,21,195},{10,19,35},{8,0,115},{8,0,51},
+        {9,0,199},{8,17,11},{8,0,99},{8,0,35},{9,0,167},{8,0,3},{8,0,131},
+        {8,0,67},{9,0,231},{7,16,7},{8,0,91},{8,0,27},{9,0,151},{11,20,67},
+        {8,0,123},{8,0,59},{9,0,215},{9,18,19},{8,0,107},{8,0,43},{9,0,183},
+        {8,0,11},{8,0,139},{8,0,75},{9,0,247},{7,16,5},{8,0,87},{8,0,23},
+        {8,64,0},{10,19,51},{8,0,119},{8,0,55},{9,0,207},{8,17,15},{8,0,103},
+        {8,0,39},{9,0,175},{8,0,7},{8,0,135},{8,0,71},{9,0,239},{7,16,9},
+        {8,0,95},{8,0,31},{9,0,159},{11,20,99},{8,0,127},{8,0,63},{9,0,223},
+        {9,18,27},{8,0,111},{8,0,47},{9,0,191},{8,0,15},{8,0,143},{8,0,79},
+        {9,0,255}
+    };
+
+    static const code distfix[32] = {
+        {5,16,1},{12,23,257},{8,19,17},{16,27,4097},{6,17,5},{14,25,1025},
+        {10,21,65},{18,29,16385},{5,16,3},{13,24,513},{9,20,33},{17,28,8193},
+        {7,18,9},{15,26,2049},{11,22,129},{5,64,0},{5,16,2},{12,23,385},
+        {8,19,25},{16,27,6145},{6,17,7},{14,25,1537},{10,21,97},{18,29,24577},
+        {5,16,4},{13,24,769},{9,20,49},{17,28,12289},{7,18,13},{15,26,3073},
+        {11,22,193},{5,64,0}
+    };
--- a/Makefile.in	1696239940.160153503
+++ b/Makefile.in	1696240243.357817402
@@ -18,17 +18,17 @@
 
 CC=cc
 
-CFLAGS=-O3 -DHAVE_HIDDEN
+CFLAGS=-O3 -DHAVE_HIDDEN -DUSE_MMAP -DVEC_OPTIMIZE -DINFFAST_OPT -std=gnu11
 #CFLAGS=-O -DMAX_WBITS=14 -DMAX_MEM_LEVEL=7
 #CFLAGS=-g -DZLIB_DEBUG
 #CFLAGS=-O3 -Wall -Wwrite-strings -Wpointer-arith -Wconversion \
 #           -Wstrict-prototypes -Wmissing-prototypes
 
-SFLAGS=-O3 -fPIC -DHAVE_HIDDEN
+SFLAGS=-O3 -fPIC -DHAVE_HIDDEN -DUSE_MMAP -DVEC_OPTIMIZE -DINFFAST_OPT -std=gnu11
 LDFLAGS=
 TEST_LDFLAGS=$(LDFLAGS) -L. libz.a
 LDSHARED=$(CC) -dynamiclib -install_name ${exec_prefix}/lib/libz.1.dylib -compatibility_version 1 -current_version 1.3.0
-CPP=$(CC) -E
+CPP=$(CC) -E -DUSE_MMAP -DVEC_OPTIMIZE -DINFFAST_OPT -std=gnu11
 
 STATICLIB=libz.a
 SHAREDLIB=libz.dylib
@@ -57,11 +57,11 @@ SRCDIR=
 ZINC=
 ZINCOUT=-I.
 
-OBJZ = adler32.o crc32.o deflate.o infback.o inffast.o inflate.o inftrees.o trees.o zutil.o
+OBJZ = adler32.o adler32vec_arm.o adler32vec_arm64.o adler32vec_intel.o crc32.o crc32lt_arm64.o crc32lt_intel.o deflate.o infback.o inffast.o inflate.o inftrees.o trees.o zutil.o zopt_inffast.o
 OBJG = compress.o uncompr.o gzclose.o gzlib.o gzread.o gzwrite.o
 OBJC = $(OBJZ) $(OBJG)
 
-PIC_OBJZ = adler32.lo crc32.lo deflate.lo infback.lo inffast.lo inflate.lo inftrees.lo trees.lo zutil.lo
+PIC_OBJZ = adler32.lo adler32vec_arm.lo adler32vec_arm64.lo adler32vec_intel.lo crc32.lo crc32lt_arm64.lo crc32lt_intel.lo deflate.lo infback.lo inffast.lo inflate.lo inftrees.lo trees.lo zutil.lo zopt_inffast.lo
 PIC_OBJG = compress.lo uncompr.lo gzclose.lo gzlib.lo gzread.lo gzwrite.lo
 PIC_OBJC = $(PIC_OBJZ) $(PIC_OBJG)
 
@@ -159,9 +159,24 @@ minigzip64.o: $(SRCDIR)test/minigzip.c $
 adler32.o: $(SRCDIR)adler32.c
 	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)adler32.c
 
+adler32vec_arm.o: $(SRCDIR)AddOn/ZAssembly/adler32vec_arm.s
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/ZAssembly/adler32vec_arm.s
+
+adler32vec_arm64.o: $(SRCDIR)AddOn/ZAssembly/adler32vec_arm64.s
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/ZAssembly/adler32vec_arm64.s
+
+adler32vec_intel.o: $(SRCDIR)AddOn/ZAssembly/adler32vec_intel.s
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/ZAssembly/adler32vec_intel.s
+
 crc32.o: $(SRCDIR)crc32.c
 	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)crc32.c
 
+crc32lt_arm64.o: $(SRCDIR)AddOn/ZAssembly/crc32lt_arm64.s
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/ZAssembly/crc32lt_arm64.s
+
+crc32lt_intel.o: $(SRCDIR)AddOn/ZAssembly/crc32lt_intel.s
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/ZAssembly/crc32lt_intel.s
+
 deflate.o: $(SRCDIR)deflate.c
 	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)deflate.c
 
@@ -183,6 +198,9 @@ trees.o: $(SRCDIR)trees.c
 zutil.o: $(SRCDIR)zutil.c
 	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)zutil.c
 
+zopt_inffast.o: $(SRCDIR)AddOn/zopt_inffast.c
+	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)AddOn/zopt_inffast.c
+
 compress.o: $(SRCDIR)compress.c
 	$(CC) $(CFLAGS) $(ZINC) -c -o $@ $(SRCDIR)compress.c
 
@@ -207,11 +225,36 @@ adler32.lo: $(SRCDIR)adler32.c
 	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/adler32.o $(SRCDIR)adler32.c
 	-@mv objs/adler32.o $@
 
+adler32vec_arm.lo: $(SRCDIR)AddOn/ZAssembly/adler32vec_arm.s
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/adler32vec_arm.o $(SRCDIR)AddOn/ZAssembly/adler32vec_arm.s
+	-@mv objs/adler32vec_arm.o $@
+
+adler32vec_arm64.lo: $(SRCDIR)AddOn/ZAssembly/adler32vec_arm64.s
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/adler32vec_arm64.o $(SRCDIR)AddOn/ZAssembly/adler32vec_arm64.s
+	-@mv objs/adler32vec_arm64.o $@
+
+adler32vec_intel.lo: $(SRCDIR)AddOn/ZAssembly/adler32vec_intel.s
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/adler32vec_intel.o $(SRCDIR)AddOn/ZAssembly/adler32vec_intel.s
+	-@mv objs/adler32vec_intel.o $@
+
 crc32.lo: $(SRCDIR)crc32.c
 	-@mkdir objs 2>/dev/null || test -d objs
 	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/crc32.o $(SRCDIR)crc32.c
 	-@mv objs/crc32.o $@
 
+crc32lt_arm64.lo: $(SRCDIR)AddOn/ZAssembly/crc32lt_arm64.s
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/crc32lt_arm64.o $(SRCDIR)AddOn/ZAssembly/crc32lt_arm64.s
+	-@mv objs/crc32lt_arm64.o $@
+
+crc32lt_intel.lo: $(SRCDIR)AddOn/ZAssembly/crc32lt_intel.s
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/crc32lt_intel.o $(SRCDIR)AddOn/ZAssembly/crc32lt_intel.s
+	-@mv objs/crc32lt_intel.o $@
+
 deflate.lo: $(SRCDIR)deflate.c
 	-@mkdir objs 2>/dev/null || test -d objs
 	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/deflate.o $(SRCDIR)deflate.c
@@ -247,6 +290,11 @@ zutil.lo: $(SRCDIR)zutil.c
 	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/zutil.o $(SRCDIR)zutil.c
 	-@mv objs/zutil.o $@
 
+zopt_inffast.lo: $(SRCDIR)AddOn/zopt_inffast.c
+	-@mkdir objs 2>/dev/null || test -d objs
+	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/zopt_inffast.o $(SRCDIR)AddOn/zopt_inffast.c
+	-@mv objs/zopt_inffast.o $@
+
 compress.lo: $(SRCDIR)compress.c
 	-@mkdir objs 2>/dev/null || test -d objs
 	$(CC) $(SFLAGS) $(ZINC) -DPIC -c -o objs/compress.o $(SRCDIR)compress.c
@@ -395,6 +443,7 @@ tags:
 
 adler32.o zutil.o: $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h
 gzclose.o gzlib.o gzread.o gzwrite.o: $(SRCDIR)zlib.h zconf.h $(SRCDIR)gzguts.h
+zopt_inffast.o: $(SRCDIR)zutil.h $(SRCDIR)inffast.h $(SRCDIR)inftrees.h $(SRCDIR)inflate.h $(SRCDIR)AddOn/zopt_inffast.h
 compress.o example.o minigzip.o uncompr.o: $(SRCDIR)zlib.h zconf.h
 crc32.o: $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h $(SRCDIR)crc32.h
 deflate.o: $(SRCDIR)deflate.h $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h
@@ -405,6 +454,7 @@ trees.o: $(SRCDIR)deflate.h $(SRCDIR)zut
 
 adler32.lo zutil.lo: $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h
 gzclose.lo gzlib.lo gzread.lo gzwrite.lo: $(SRCDIR)zlib.h zconf.h $(SRCDIR)gzguts.h
+zopt_inffast.lo: $(SRCDIR)zutil.h $(SRCDIR)inffast.h $(SRCDIR)inftrees.h $(SRCDIR)inflate.h $(SRCDIR)AddOn/zopt_inffast.h
 compress.lo example.lo minigzip.lo uncompr.lo: $(SRCDIR)zlib.h zconf.h
 crc32.lo: $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h $(SRCDIR)crc32.h
 deflate.lo: $(SRCDIR)deflate.h $(SRCDIR)zutil.h $(SRCDIR)zlib.h zconf.h
--- a/configure	1692348336.000000000
+++ b/configure	1696241374.862102066
@@ -826,8 +826,8 @@ int main()
 }
 EOF
   if tryboth $CC -c $CFLAGS $test.c; then
-    CFLAGS="$CFLAGS -DHAVE_HIDDEN"
-    SFLAGS="$SFLAGS -DHAVE_HIDDEN"
+    CFLAGS="$CFLAGS -DHAVE_HIDDEN -DUSE_MMAP -DVEC_OPTIMIZE -DINFFAST_OPT -std=gnu11"
+    SFLAGS="$SFLAGS -DHAVE_HIDDEN -DUSE_MMAP -DVEC_OPTIMIZE -DINFFAST_OPT -std=gnu11"
     echo "Checking for attribute(visibility) support... Yes." | tee -a configure.log
   else
     echo "Checking for attribute(visibility) support... No." | tee -a configure.log
--- a/adler32.c	1696236132.830069763
+++ b/adler32.c	1696241659.532139025
@@ -7,6 +7,10 @@
 
 #include "zutil.h"
 
+#if defined(VEC_OPTIMIZE)
+#include "AddOn/zopt_inffast.h"
+#endif
+
 #define BASE 65521U     /* largest prime smaller than 65536 */
 #define NMAX 5552
 /* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
--- a/crc32.c	1696236132.831188871
+++ b/crc32.c	1696241896.385114609
@@ -29,8 +29,11 @@
 
 #include "zutil.h"      /* for Z_U4, Z_U8, z_crc_t, and FAR definitions */
 
-#if defined(VEC_OPTIMIZE) && defined(__x86_64__)
+#if defined(VEC_OPTIMIZE)
+#  include "AddOn/zopt_inffast.h"
+#  if defined(__x86_64__)
 #include <machine/cpu_capabilities.h>
+#  endif
 #endif
 
  /*
--- a/infback.c	1696236132.833677500
+++ b/infback.c	1696241962.630830158
@@ -15,6 +15,10 @@
 #include "inflate.h"
 #include "inffast.h"
 
+#if defined(INFFAST_OPT)
+#  include "AddOn/zopt_inffast.h"
+#endif
+
 /*
    strm provides memory allocation functions in zalloc and zfree, or
    Z_NULL to use the library memory allocation functions.
--- a/inflate.c	1696236132.835265643
+++ b/inflate.c	1696242068.949144813
@@ -91,6 +91,10 @@
 #  endif
 #endif
 
+#if defined(INFFAST_OPT)
+#  include "AddOn/zopt_inffast.h"
+#endif
+
 local int inflateStateCheck(z_streamp strm) {
     struct inflate_state FAR *state;
     if (strm == Z_NULL ||
--- a/inftrees.c	1696236132.836247920
+++ b/inftrees.c	1696242204.222578894
@@ -6,6 +6,10 @@
 #include "zutil.h"
 #include "inftrees.h"
 
+#if defined(INFFAST_OPT)
+#  include "AddOn/zopt_inffast.h"
+#endif
+
 #define MAXBITS 15
 
 const char inflate_copyright[] =
--- a/inffast.c	1681618651.000000000
+++ b/inffast.c	1696242522.177250680
@@ -8,6 +8,10 @@
 #include "inflate.h"
 #include "inffast.h"
 
+#if defined(INFFAST_OPT)
+#  include "AddOn/zopt_inffast.h"
+#endif
+
 #ifdef ASMINF
 #  pragma message("Assembler code may have bugs -- use at your own risk")
 #else
